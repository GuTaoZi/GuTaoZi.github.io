<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-32x32.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"gutaozi.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.27.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="AI这东西随主人的，我学不进去它也学不进去">
<meta property="og:type" content="article">
<meta property="og:title" content="CS311 人工智能 期末复习">
<meta property="og:url" content="https://gutaozi.github.io/2026/01/11/CS311_Final_Review/index.html">
<meta property="og:site_name" content="GuTao&#39;s Nest">
<meta property="og:description" content="AI这东西随主人的，我学不进去它也学不进去">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/TnYsVXi8B56aeuJ.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/DzvuyEiZMP172UW.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/LYKqpsglET7nU3W.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/xFzXuBYUosTMe4w.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/14/tpjgqJIMOQrNUAb.png">
<meta property="article:published_time" content="2026-01-11T08:44:54.769Z">
<meta property="article:modified_time" content="2026-01-11T08:44:54.821Z">
<meta property="article:author" content="Gu Tao">
<meta property="article:tag" content="Artificial Intelligence">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/06/13/TnYsVXi8B56aeuJ.png">


<link rel="canonical" href="https://gutaozi.github.io/2026/01/11/CS311_Final_Review/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://gutaozi.github.io/2026/01/11/CS311_Final_Review/","path":"2026/01/11/CS311_Final_Review/","title":"CS311 人工智能 期末复习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CS311 人工智能 期末复习 | GuTao's Nest</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">GuTao's Nest</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">Chapter 1 - 人工智能概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-number">1.1.</span> <span class="nav-text">智能体</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83"><span class="nav-number">1.2.</span> <span class="nav-text">环境</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-2-%E6%90%9C%E7%B4%A2"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2 - 搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E6%A6%82%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">搜索概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E4%BF%A1%E6%81%AF%E6%90%9C%E7%B4%A2"><span class="nav-number">2.2.</span> <span class="nav-text">无信息搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E4%BF%A1%E6%81%AF%E6%90%9C%E7%B4%A2"><span class="nav-number">2.3.</span> <span class="nav-text">有信息搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83BFS"><span class="nav-number">2.3.1.</span> <span class="nav-text">贪心BFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-%E6%90%9C%E7%B4%A2"><span class="nav-number">2.3.2.</span> <span class="nav-text">A*搜索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E6%8E%A5%E5%8F%97%E7%9A%84%E5%90%AF%E5%8F%91%E5%87%BD%E6%95%B0-Admissible-heuristics"><span class="nav-number">2.3.3.</span> <span class="nav-text">可接受的启发函数(Admissible heuristics)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#IDA-%E6%90%9C%E7%B4%A2"><span class="nav-number">2.3.4.</span> <span class="nav-text">IDA*搜索</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2"><span class="nav-number">2.4.</span> <span class="nav-text">局部搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%88%AC%E5%B1%B1%E7%AE%97%E6%B3%95"><span class="nav-number">2.4.1.</span> <span class="nav-text">爬山算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="nav-number">2.4.2.</span> <span class="nav-text">遗传算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB"><span class="nav-number">2.4.3.</span> <span class="nav-text">模拟退火</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-3-%E5%90%AF%E5%8F%91%E4%B8%8E%E5%85%83%E5%90%AF%E5%8F%91"><span class="nav-number">3.</span> <span class="nav-text">Chapter 3 - 启发与元启发</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6"><span class="nav-number">3.1.</span> <span class="nav-text">启发式函数进阶</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E4%B8%8E%E6%95%88%E7%8E%87"><span class="nav-number">3.1.1.</span> <span class="nav-text">启发式函数与效率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%BD%E7%9A%84%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E4%B8%8A%E5%93%AA%E6%89%BE%EF%BC%9F"><span class="nav-number">3.1.2.</span> <span class="nav-text">好的启发式函数上哪找？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%83%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6"><span class="nav-number">3.2.</span> <span class="nav-text">元启发式函数进阶</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-4-%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2"><span class="nav-number">4.</span> <span class="nav-text">Chapter 4 - 对抗搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Minimax%E6%90%9C%E7%B4%A2"><span class="nav-number">4.1.</span> <span class="nav-text">Minimax搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alpha-beta-%E5%89%AA%E6%9E%9D"><span class="nav-number">4.2.</span> <span class="nav-text">$\alpha-\beta$ 剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E8%A6%81%E5%89%AA%E5%91%A2%EF%BC%9F"><span class="nav-number">4.2.1.</span> <span class="nav-text">什么时候要剪呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%80%9C%E8%87%AA%E8%A3%81%E5%BC%8F%E2%80%9D%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.2.2.</span> <span class="nav-text">“自裁式”实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%A2%E5%BA%8F%E5%89%AA%E6%9E%9D"><span class="nav-number">4.2.3.</span> <span class="nav-text">换序剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E6%83%85%E5%86%B5"><span class="nav-number">4.2.4.</span> <span class="nav-text">实际情况</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9BMinimax%E6%90%9C%E7%B4%A2"><span class="nav-number">4.3.</span> <span class="nav-text">期望Minimax搜索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-5-%E7%BA%A6%E6%9D%9F%E6%BB%A1%E8%B6%B3%E9%97%AE%E9%A2%98"><span class="nav-number">5.</span> <span class="nav-text">Chapter 5 - 约束满足问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CSP%E6%A6%82%E8%BF%B0"><span class="nav-number">5.1.</span> <span class="nav-text">CSP概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3CSP"><span class="nav-number">5.2.</span> <span class="nav-text">解决CSP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9BBTS%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-number">5.3.</span> <span class="nav-text">改进BTS的三种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E5%89%A9%E4%BD%99%E5%80%BC-Minimum-Remaining-Value"><span class="nav-number">5.3.1.</span> <span class="nav-text">最小剩余值(Minimum Remaining Value)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E9%99%90%E5%88%B6%E5%80%BC-Least-Constraining-Value"><span class="nav-number">5.3.2.</span> <span class="nav-text">最小限制值(Least Constraining Value)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E6%A3%80%E6%9F%A5-Forward-Checking"><span class="nav-number">5.3.3.</span> <span class="nav-text">前向检查(Forward Checking)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">5.4.</span> <span class="nav-text">一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%82%B9%E4%B8%80%E8%87%B4%E6%80%A7-%E4%B8%80%E5%85%83%E9%99%90%E5%88%B6"><span class="nav-number">5.4.1.</span> <span class="nav-text">点一致性(一元限制)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A7%E4%B8%80%E8%87%B4%E6%80%A7-%E4%BA%8C%E5%85%83%E9%99%90%E5%88%B6"><span class="nav-number">5.4.2.</span> <span class="nav-text">弧一致性(二元限制)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E4%B8%80%E8%87%B4%E6%80%A7-%E5%A4%9A%E5%85%83%E9%99%90%E5%88%B6"><span class="nav-number">5.4.3.</span> <span class="nav-text">路径一致性(多元限制)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-6-%E9%80%BB%E8%BE%91"><span class="nav-number">6.</span> <span class="nav-text">Chapter 6 - 逻辑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%93%BA%E5%9E%AB"><span class="nav-number">6.1.</span> <span class="nav-text">铺垫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Resolution"><span class="nav-number">6.2.</span> <span class="nav-text">Resolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-Backward-chaining"><span class="nav-number">6.3.</span> <span class="nav-text">Forward &#x2F; Backward chaining</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-7-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-number">7.</span> <span class="nav-text">Chapter 7 - 机器学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.1.</span> <span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.2.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K%E8%BF%91%E9%82%BB"><span class="nav-number">7.3.</span> <span class="nav-text">K近邻</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-Validation-and-Test"><span class="nav-number">7.4.</span> <span class="nav-text">Train, Validation and Test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">7.5.</span> <span class="nav-text">过拟合和欠拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-Validation-and-Test-1"><span class="nav-number">7.6.</span> <span class="nav-text">Train, Validation and Test</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-fold%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">7.6.1.</span> <span class="nav-text">K-fold交叉验证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5-Confusion-matrix"><span class="nav-number">7.6.2.</span> <span class="nav-text">混淆矩阵(Confusion matrix)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%AF%B9%E6%95%B0%E5%9B%9E%E5%BD%92"><span class="nav-number">8.</span> <span class="nav-text">Chapter 8 - 线性回归与对数回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">8.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="nav-number">8.1.1.</span> <span class="nav-text">正规方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">8.1.2.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E8%80%83%E8%99%91"><span class="nav-number">8.1.3.</span> <span class="nav-text">实际考虑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E5%9B%9E%E5%BD%92"><span class="nav-number">8.2.</span> <span class="nav-text">对数回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-9-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">9.</span> <span class="nav-text">Chapter 9 - 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.1.</span> <span class="nav-text">线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.2.</span> <span class="nav-text">非线性模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-10-%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">10.</span> <span class="nav-text">Chapter 10 - 感知机与神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">10.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">10.2.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="nav-number">10.2.1.</span> <span class="nav-text">反向传播算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-11-%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">11.</span> <span class="nav-text">Chapter 11 - 决策树和朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">11.1.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">11.2.</span> <span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="nav-number">11.2.1.</span> <span class="nav-text">判别模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">11.2.2.</span> <span class="nav-text">生成模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-12-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%81%9A%E7%B1%BB"><span class="nav-number">12.</span> <span class="nav-text">Chapter 12 - 集成学习与聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">12.1.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Boosting"><span class="nav-number">12.1.1.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bagging"><span class="nav-number">12.1.2.</span> <span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">12.1.3.</span> <span class="nav-text">随机森林</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">12.2.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means"><span class="nav-number">12.2.1.</span> <span class="nav-text">K-means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-number">12.2.2.</span> <span class="nav-text">K-means的一些问题</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Gu Tao"
      src="https://avatars.githubusercontent.com/u/109007949?v=4">
  <p class="site-author-name" itemprop="name">Gu Tao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/gutaozi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;gutaozi" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:gutao.official@gmail.com" title="E-Mail → mailto:gutao.official@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://gutaozi.github.io/2026/01/11/CS311_Final_Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/109007949?v=4">
      <meta itemprop="name" content="Gu Tao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GuTao's Nest">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CS311 人工智能 期末复习 | GuTao's Nest">
      <meta itemprop="description" content="AI这东西随主人的，我学不进去它也学不进去">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS311 人工智能 期末复习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2026-01-11 16:44:54" itemprop="dateCreated datePublished" datetime="2026-01-11T16:44:54+08:00">2026-01-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">AI这东西随主人的，我学不进去它也学不进去</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>$\huge\text{Outline}$</p>
<ol>
<li>人工智能概述</li>
<li>搜索</li>
<li>启发与元启发</li>
<li>对抗搜索</li>
<li>约束满足问题</li>
<li>逻辑</li>
<li>机器学习概述</li>
<li>线性回归与对数回归</li>
<li>支持向量机</li>
<li>感知机与神经网络</li>
<li>决策树与朴素贝叶斯</li>
<li>集成学习与聚类</li>
<li>强化学习</li>
<li>自然语言处理</li>
</ol>
<hr>
<h2 id="Chapter-1-人工智能概述"><a href="#Chapter-1-人工智能概述" class="headerlink" title="Chapter 1 - 人工智能概述"></a>Chapter 1 - 人工智能概述</h2><p>第一课略，大致是概念的介绍和AI发展的介绍。</p>
<h3 id="智能体"><a href="#智能体" class="headerlink" title="智能体"></a>智能体</h3><p><strong>智能体的特征</strong></p>
<ul>
<li><p>通过探测器感知环境</p>
<p>perceiving its environment through sensors</p>
</li>
<li><p>根据环境利用执行器执行操作</p>
<p>acting upon that environment through actuators</p>
</li>
</ul>
<p>智能体运行周期：感知(perceive)，思考(perceive)，行动(act)</p>
<p>智能体是Architecture和Program的互补兼容。</p>
<p><strong>Rationality的评价标准：PEAS</strong></p>
<ul>
<li>Performance</li>
<li>Environment</li>
<li>Actuators</li>
<li>Sensors</li>
</ul>
<p><strong>智能体的类型</strong></p>
<ul>
<li><p>Simple reflex agents</p>
<p>只根据当前状态决策行动，需要完全可观测环境，且正确行动只与当前观测有关。</p>
</li>
<li><p>Model-based reflex agents</p>
<p>智能机内部状态由历史观测决定，可以处理部分可观测环境</p>
<p>建立环境模型：环境如何独立演变 + 行动如何影响环境</p>
</li>
<li><p>Goal-based agents</p>
<p>拥有目标信息，考虑每种行动对接近目标的贡献</p>
</li>
<li><p>Utility-based agents</p>
<p>Utility function评估智能体性能，选择最大化期望utility的行动</p>
</li>
<li><p>Learning agents</p>
<p>Learning element：根据输入进行提升</p>
<p>Performance element：选择执行的行动</p>
<p>Critic：由固定的性能评估标准决定智能体的表现</p>
<p>Problem Generator：允许智能体进行探索</p>
</li>
</ul>
<p><strong>智能体的状态</strong></p>
<ul>
<li><p>原子表示Atomic Representation</p>
<p>每个环境状态都是不具有内部结构的黑盒</p>
<p>Search, games, Markov decision processes, hidden Markov models, etc</p>
</li>
<li><p>因子表示Factored Representation</p>
<p>每个状态都有一些属性，可以用一组变量来表示</p>
<p> Constraint satisfaction, and Bayesian networks. </p>
</li>
<li><p>结构表示Structured Representation</p>
<p>状态之间的关系可以显式表达</p>
<p>First order logic, knowledge-based learning, natural language understanding.</p>
</li>
</ul>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><strong>环境的类型</strong></p>
<ul>
<li><p>完全可观测Fully observable&#x2F;部分可观测Partially observable</p>
</li>
<li><p>确定Deterministic&#x2F;随机Stochastic</p>
<p>下一步的环境是否只由Agent的行动和当前环境决定。</p>
</li>
<li><p>Episodic&#x2F;Sequential</p>
<p>前者说明每一步行动不影响下一步，后者说明会影响。</p>
</li>
<li><p>静态Static&#x2F;动态Dynamic</p>
<p>环境在智能体运行时是否变化(半动态：环境不变，但评分变动)</p>
</li>
<li><p>离散Discrete&#x2F;连续Continuous</p>
</li>
<li><p>单智能体Single-agent&#x2F;多智能体Multi-agent</p>
</li>
<li><p>已知Known&#x2F;未知Unknown</p>
<p>设计者是否拥有对环境的知识</p>
</li>
</ul>
<h2 id="Chapter-2-搜索"><a href="#Chapter-2-搜索" class="headerlink" title="Chapter 2 - 搜索"></a>Chapter 2 - 搜索</h2><h3 id="搜索概述"><a href="#搜索概述" class="headerlink" title="搜索概述"></a>搜索概述</h3><p><strong>通过搜索解决问题</strong></p>
<ol>
<li><p>定义问题</p>
<p>(a)目标形式化</p>
<p>(b)问题形式化</p>
</li>
<li><p>两步解决问题</p>
<p>(a)离线搜索不同行动</p>
<p>(b)执行搜索结果</p>
</li>
</ol>
<p><strong>问题形式化</strong></p>
<ul>
<li>初始状态</li>
<li>状态：所有由初始状态，通过任意行动序列可达的状态集合(状态空间)</li>
<li>行动：对于一个状态，智能体可以执行的所有行动(行动空间)</li>
<li>转化模型：描述每个行动对当前状态造成的影响</li>
<li>目标检测：决定给定状态是否已经达到目标</li>
<li>路径开销：根据性能标准为动作序列赋上数值开销的函数</li>
</ul>
<p><strong>状态空间 vs. 搜索空间</strong></p>
<ul>
<li><p>状态空间是实际状态的集合</p>
</li>
<li><p>搜索空间是抽象成搜索树&#x2F;图的可行解集合</p>
</li>
<li><p>搜索树描述了行动序列</p>
<ul>
<li><p>根节点：初始状态</p>
</li>
<li><p>分支：行动</p>
</li>
<li><p>结点：行动的结果</p>
<p>每个结点有：父节点，子节点，深度，路径开销，在状态空间对应的状态</p>
</li>
</ul>
</li>
<li><p>扩张Expand：对于给定结点，创建所有子结点的函数</p>
</li>
</ul>
<p><strong>搜索空间的三个区域</strong></p>
<ol>
<li>Explored (a.k.a. Closed List, Visited Set) </li>
<li>Frontier  (a.k.a. Ready list, Open List, the Fringe) </li>
<li>Unexplored</li>
</ol>
<p>搜索就是从3到2再到1的过程，搜索策略决定了顺序。</p>
<p><strong>搜索策略</strong></p>
<p>搜索策略由选定结点进行扩张的顺序来定义，评价标准：</p>
<ul>
<li>完备性Completeness</li>
<li>时间复杂度Time complexity</li>
<li>空间复杂度Space complexity</li>
<li>最优性Optimality</li>
</ul>
<p>时间复杂度和空间复杂度的评估：</p>
<ul>
<li>$b$ - 搜索树最大的分支数量</li>
<li>$d$ - 解的深度</li>
<li>$m$ - 状态空间的最大深度</li>
</ul>
<h3 id="无信息搜索"><a href="#无信息搜索" class="headerlink" title="无信息搜索"></a>无信息搜索</h3><p>不使用domain knowledge的搜索。</p>
<ul>
<li><p>Breadth-first search &#x2F; 广度优先 &#x2F; BFS</p>
<p>优先探索最浅的结点</p>
</li>
<li><p>Depth-first search &#x2F; 深度优先 &#x2F; DFS</p>
<p>优先探索最深的结点</p>
</li>
<li><p>Depth-limited search &#x2F; 深度限制 &#x2F; DLS</p>
<p>带深度限制的DFS</p>
</li>
<li><p>Iterative-deepening search &#x2F; 迭代加深 &#x2F; IDS</p>
<p>不断放宽深度限制(提高深度上限)的DLS</p>
</li>
<li><p>Uniform-cost search (UCS) &#x2F; 统一代价 &#x2F; UCS</p>
<p>优先探索开销最小的结点（参考Dijkstra）</p>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>完备性</th>
<th>时间</th>
<th>空间</th>
<th>最优性</th>
<th>实现</th>
</tr>
</thead>
<tbody><tr>
<td>BFS</td>
<td>有限分支则完备</td>
<td>$O(b^d)$</td>
<td>$O(b^d)$</td>
<td>每步开销都为1则最优</td>
<td>Queue</td>
</tr>
<tr>
<td>DFS</td>
<td>有限状态空间则完备</td>
<td>$O(b^m)$</td>
<td>$O(bm)$</td>
<td>否</td>
<td>Stack</td>
</tr>
<tr>
<td>UCS</td>
<td>有限开销则完备</td>
<td>$O(b^{C^*&#x2F;\epsilon})$</td>
<td>$O(b^{C^*&#x2F;\epsilon})$</td>
<td>是</td>
<td>Heap</td>
</tr>
</tbody></table>
<p>* 其中$C^*$是最优解的开销，每一步行动至少开销$\epsilon$</p>
<h3 id="有信息搜索"><a href="#有信息搜索" class="headerlink" title="有信息搜索"></a>有信息搜索</h3><p>使用领域知识来获取关于当前状态与目标距离的信息。</p>
<h4 id="贪心BFS"><a href="#贪心BFS" class="headerlink" title="贪心BFS"></a>贪心BFS</h4><p>把到目标的估计费用$h(n)$当作唯一标准贪心搜索</p>
<h4 id="A-搜索"><a href="#A-搜索" class="headerlink" title="A*搜索"></a>A*搜索</h4><p>$f(n)&#x3D;$到当前结点的费用$g(n)$+当前结点到目标的估计费用$h(n)$</p>
<p>$f(n)$是经过结点$n$的最优路径费用的估值</p>
<p>完备性：是</p>
<p>时间复杂度：指数级</p>
<p>空间复杂度：每个结点都在内存里，最大的问题</p>
<p>最优性：是</p>
<h4 id="可接受的启发函数-Admissible-heuristics"><a href="#可接受的启发函数-Admissible-heuristics" class="headerlink" title="可接受的启发函数(Admissible heuristics)"></a>可接受的启发函数(Admissible heuristics)</h4><p>如果启发式函数从不高估到达目标的费用，那么就是可接受的(admissible)，并且使用该启发式函数的A*得到的解是最优的。</p>
<p>证明：</p>
<ul>
<li><p>Suppose $G_o$ is the optimal goal.</p>
<p>Suppose $G_s$ is some suboptimal goal.</p>
<p>Suppose $n$ is on the shortest path to $G_o$. </p>
</li>
<li><p>$f(G_s) &#x3D; g(G_s)$ since $h(G_s) &#x3D; 0$</p>
<p>$f(G_o ) &#x3D; g(G_o )$ since $h(G_o ) &#x3D; 0$</p>
<p>$g(G_s)&gt;g(G_o )$ since $G_s$ is suboptimal</p>
</li>
<li><p>Then $f(G_s) &gt; f(G_o ) . . . (1)$</p>
<p>$h(n)\le h^*(n)$ since h is admissible</p>
<p>$g(n)+h(n)\le g(n)+h^* (n) &#x3D; g(G_o ) &#x3D; f(G_o )$</p>
<p>Then $f(n) \le f(G_o ) . . . (2)$</p>
</li>
<li><p>From (1) and (2) $f(G_s)&gt;f(n)$, so A* will never select $G_s$ during the search and hence A* is optimal.</p>
</li>
</ul>
<h4 id="IDA-搜索"><a href="#IDA-搜索" class="headerlink" title="IDA*搜索"></a>IDA*搜索</h4><p>把迭代加深方法加到A*搜索上。</p>
<h3 id="局部搜索"><a href="#局部搜索" class="headerlink" title="局部搜索"></a>局部搜索</h3><p>不关心到目标的路径，优化问题可以使用局部搜索。</p>
<p>只维护当前状态，并通过移动到相邻状态，试图优化。</p>
<ul>
<li>不需要维护搜索树</li>
<li>内存占用小</li>
<li>在连续或较大的状态空间中找到较好解</li>
</ul>
<h4 id="爬山算法"><a href="#爬山算法" class="headerlink" title="爬山算法"></a>爬山算法</h4><p>贪心局部搜索，只选择最优的邻居进行探索，到达峰值时停止(可能是局部峰值)</p>
<p>每个结点代表一个状态和一个值</p>
<p>变种：</p>
<ul>
<li>Sideways moves 允许转移到与当前状态同等优的解</li>
<li>Random-restart 多试几次取最优</li>
<li>Stochastic 随机选择不同的上坡算子</li>
</ul>
<p>改进：</p>
<ul>
<li>Hill climbing：取决于地形，多重开几次还是不错的</li>
<li>Local beam search：同时维护前k优的解，探索其邻居</li>
<li>Stochastic beam search：随机选择k个解(不一定是前k优)，探索其邻居</li>
</ul>
<h4 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h4><p>Stochastic beam search的变种。</p>
<p>自然选择，种群，个体，基因，健壮函数，交叉互换，后代，变异</p>
<h4 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h4><p>温度指数级下降，以一定概率接受较劣解，温度越高跳出当前解的可能性越高。</p>
<p>接受概率：$P &#x3D; e^{-\Delta E&#x2F;T}$</p>
<h2 id="Chapter-3-启发与元启发"><a href="#Chapter-3-启发与元启发" class="headerlink" title="Chapter 3 - 启发与元启发"></a>Chapter 3 - 启发与元启发</h2><h3 id="启发式函数进阶"><a href="#启发式函数进阶" class="headerlink" title="启发式函数进阶"></a>启发式函数进阶</h3><h4 id="启发式函数与效率"><a href="#启发式函数与效率" class="headerlink" title="启发式函数与效率"></a>启发式函数与效率</h4><p>如果一个启发式函数始终不小于另一个启发式函数，称其为dominate，前者拥有更好的效率。好的启发式函数可以降低$b^\star$，分支数量化了启发式搜索的效率。</p>
<p>对于一堆admissible的启发式函数${h_i}$，能够dominate所有函数的是$\max{h_i}$。</p>
<h4 id="好的启发式函数上哪找？"><a href="#好的启发式函数上哪找？" class="headerlink" title="好的启发式函数上哪找？"></a>好的启发式函数上哪找？</h4><ol>
<li>从放宽条件的问题入手，例如8-puzzle允许块之间重叠，得到的步数总是要比原问题少的。</li>
<li>从子问题入手，例如8-puzzle只把一部分块归位而不管其他的。该问题中，函数$h_{\text{sub}}$(只考虑四块的启发式函数极大值并集)是dominate函数$h_{1step}$的。</li>
<li>根据经验生成，可以选定当前状态的几个特征作为变量，用神经网络&#x2F;决策树来学这些变量对应的权重，用线性函数表示启发式函数</li>
</ol>
<h3 id="元启发式函数进阶"><a href="#元启发式函数进阶" class="headerlink" title="元启发式函数进阶"></a>元启发式函数进阶</h3><p>与问题相互独立的启发式方法，可以组合使用。</p>
<blockquote>
<p>One general law, leading to the advancement of all organic beings, namely, multiply, vary, let the strongest live and weakest die.</p>
<p>Charles Darwin, The Origin of Species</p>
</blockquote>
<p>演化计算：繁殖，变异，选择</p>
<h2 id="Chapter-4-对抗搜索"><a href="#Chapter-4-对抗搜索" class="headerlink" title="Chapter 4 - 对抗搜索"></a>Chapter 4 - 对抗搜索</h2><h3 id="Minimax搜索"><a href="#Minimax搜索" class="headerlink" title="Minimax搜索"></a>Minimax搜索</h3><p>其中某一方，按照双方都足够聪明的情况进行DFS(DLS)，从搜索树所有分支中选择对各自最优的。</p>
<p>在可以搜索到终局的情况下，对于一个状态$s$，minimax函数：</p>
<p>$$f(s)&#x3D;\left{\begin{align*}&amp;\text{Utility(s)}&amp;\text{if Terminal-test(s)}\ &amp;\max_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&amp;\text{if Player(s)&#x3D;MAX}\&amp;\min_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&amp;\text{if Player(s)&#x3D;MIN}\end{align*}\right.$$</p>
<p>然而博弈是有时限的，指数级时间复杂度不可能搜完，所以：</p>
<ul>
<li>将终局评估改成非终局评估</li>
<li>使用迭代加深搜索IDS</li>
<li>剪枝</li>
</ul>
<h3 id="alpha-beta-剪枝"><a href="#alpha-beta-剪枝" class="headerlink" title="$\alpha-\beta$ 剪枝"></a>$\alpha-\beta$ 剪枝</h3><p>$\alpha$：$\text{MAX}$结点目前已经探索的结点中最大的一个。当前结点不会探索比$\alpha$估值还低的结点。</p>
<p>$\beta$：$\text{MIN}$结点目前已经探索的节点中最小的一个。当前结点不会探索比$\beta$估值还高的结点。</p>
<h4 id="什么时候要剪呢？"><a href="#什么时候要剪呢？" class="headerlink" title="什么时候要剪呢？"></a>什么时候要剪呢？</h4><p>$\text{MAX}$视角：$\text{MIN}$在再下一步的结点中，发现了比$\alpha$还低的值，那$\text{Min}$肯定会选这个分支，导致$\text{MAX}$的估值小于$\alpha$，为了避免这种局面，当下一层的$\text{MIN}$发现了一个比$\alpha$还小的值时，说明对手足够聪明的情况下，$\text{MAX}$在这一分支会被对手限制到不如$\alpha$的局面，剪掉。</p>
<p>$\text{MIN}$视角：$\text{MAX}$在再下一步的结点中，发现了比$\beta$还大的值，那$\text{MAX}$肯定会选这个分支，导致$\text{MIN}$的估值大于$\beta$，为了避免这种局面，当下一层的$\text{MAX}$发现了一个比$\beta$还大的值时，说明对手足够聪明的情况下，会选择这个新的分支规避$\beta$的限制，故剪掉。</p>
<h4 id="“自裁式”实现"><a href="#“自裁式”实现" class="headerlink" title="“自裁式”实现"></a>“自裁式”实现</h4><p>上面的说法更像是当前结点主动剪子结点。但还有一种“自裁式”的剪法，就是某个结点发现自己太好了，肯定会被足够聪明的上一层剪掉，就自裁了。</p>
<p>搜索的时候，父结点把自己的$\alpha-\beta$给子结点，探索完一个子结点后，做如下操作：</p>
<p>当前节点是$\text{MAX}$，如果$\text{MIN}$子结点给出的估值比当前最高的还高，那就检查有没有超过当前节点的$\beta$，如果超过了当前节点的$\beta$，说明当前这一支太好了，会被更高层的$\text{MIN}$掐掉，所以这个$\text{MAX}$结点就被整个剪掉了；如果没超过$\beta$而且还比当前最高还高，那就更新当前的最高值和$\alpha$。</p>
<p>当前节点是$\text{MIN}$时同理，如果这个$\text{MIN}$结点能把最优解掐到不足$\alpha$，那$\text{MIN}$结点的$\text{MAX}$父节点肯定不乐意啊，所以一整个$\text{MIN}$结点就被剪掉了。</p>
<p><img src="https://s2.loli.net/2023/06/13/TnYsVXi8B56aeuJ.png" alt="image.png"></p>
<center>Minimax搜索伪代码</center>

<h4 id="换序剪枝"><a href="#换序剪枝" class="headerlink" title="换序剪枝"></a>换序剪枝</h4><p>$\alpha-\beta$ 剪枝的效果受访问顺序影响，有可能导致剪枝的结点最后才被访问，这就剪了个寂寞。</p>
<p>最差的顺序相当于没有剪枝，复杂度是$O(b^m)$。</p>
<p>最好的顺序则是把最“聪明”的结点都放在最先访问，实际复杂度是$O(b^{m&#x2F;2})$，直接开了个根号。</p>
<p>如何找到一个好的顺序呢？除了动用领域知识，还可以保存历史表，以前发现最好的几步可能在之后还是最好的几步，或是保存状态以供重复时使用。</p>
<h4 id="实际情况"><a href="#实际情况" class="headerlink" title="实际情况"></a>实际情况</h4><p>上面的$\alpha-\beta$剪枝伪代码还是要一直搜到叶子结点，依然是不现实的，所以将$\text{Util(s)}$函数更换为可以评价中局的启发式函数$\text{eval(s)}$。</p>
<h3 id="期望Minimax搜索"><a href="#期望Minimax搜索" class="headerlink" title="期望Minimax搜索"></a>期望Minimax搜索</h3><p>对于带有随机性的博弈，期望Minimax搜索在$\text{MIN}$和$\text{MAX}$之外又加入了$\text{Chance}$层，会返回当前所有可能发生的情况合起来的期望。</p>
<p>$$f(s)&#x3D;\left{\begin{align*}&amp;\text{Utility(s)}&amp;\text{if Terminal-test(s)}\ &amp;\max_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&amp;\text{if Player(s)&#x3D;MAX}\&amp;\min_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&amp;\text{if Player(s)&#x3D;MIN}\&amp;\sum\limits_{r}P(r)f(\text{Result(s,r)})&amp;\text{if Player(s)&#x3D;Chance}\end{align*}\right.$$</p>
<p>其中$r$代表了所有的随机事件。</p>
<h2 id="Chapter-5-约束满足问题"><a href="#Chapter-5-约束满足问题" class="headerlink" title="Chapter 5 - 约束满足问题"></a>Chapter 5 - 约束满足问题</h2><h3 id="CSP概述"><a href="#CSP概述" class="headerlink" title="CSP概述"></a>CSP概述</h3><p>也是搜索问题，但是我们更关心目标本身。</p>
<p>CSP的状态是因子表示的，也就是由一组变量表示。</p>
<p>一个目标测试(goal test)是对于这组变量(的子集)的一组限制。</p>
<p><strong>CSP问题的三个要素</strong></p>
<ol>
<li>变量集合$X$</li>
<li>变量定义域的集合$D$</li>
<li>约束的集合$C$</li>
</ol>
<p>CSP的解：在定义域$D$中，对变量$X$找到一个合适的赋值方案$s$，满足所有的约束$C$。这样的解称为一致的赋值(consistent assignment)。</p>
<p>变量类型：离散&#x2F;连续</p>
<p>限制类型：一元(unary)，二元(binary)，全局(global)，偏好&#x2F;软限制(preferences&#x2F;soft constraints)</p>
<h3 id="解决CSP"><a href="#解决CSP" class="headerlink" title="解决CSP"></a>解决CSP</h3><p>暴力太慢，配合推理(inference)和搜索(search)来加速。</p>
<p>Inference主要是约束传播(constraint propagation)，是指一个变量的合法取值变少时，其他变量的合法取值也会变少。</p>
<p>Search的DFS和BFS都是老朋友了，新来了个BTS。</p>
<p>回溯搜索(Backtracking search, BTS)也是一种DFS，满足以下条件：</p>
<ul>
<li>一次赋一个变量的值：赋值顺序是可交换的</li>
<li>赋值的时候就注意和之前的赋值不能冲突</li>
</ul>
<p><del>感觉和DFS没区别啊&#x3D; &#x3D;</del></p>
<h3 id="改进BTS的三种方法"><a href="#改进BTS的三种方法" class="headerlink" title="改进BTS的三种方法"></a>改进BTS的三种方法</h3><h4 id="最小剩余值-Minimum-Remaining-Value"><a href="#最小剩余值-Minimum-Remaining-Value" class="headerlink" title="最小剩余值(Minimum Remaining Value)"></a>最小剩余值(Minimum Remaining Value)</h4><p>Q：先给哪个变量赋值？</p>
<blockquote>
<p>先挑硬柿子捏</p>
</blockquote>
<p>每次挑剩余选择最少的结点赋值。</p>
<h4 id="最小限制值-Least-Constraining-Value"><a href="#最小限制值-Least-Constraining-Value" class="headerlink" title="最小限制值(Least Constraining Value)"></a>最小限制值(Least Constraining Value)</h4><p>Q：给这个变量赋哪个值？</p>
<blockquote>
<p>少给后人添麻烦</p>
</blockquote>
<p>每次挑对别人限制最少的结点赋值。</p>
<h4 id="前向检查-Forward-Checking"><a href="#前向检查-Forward-Checking" class="headerlink" title="前向检查(Forward Checking)"></a>前向检查(Forward Checking)</h4><p>Q：世界末日是可预见的吗？</p>
<blockquote>
<p>关心每个人的未来</p>
</blockquote>
<p>每次赋值后检查所有未赋值变量定义域是否为空。</p>
<p>在前向检查的时候用到了约束传播的思想，用已赋值的变量去限制未赋值的变量，看还有没有合法赋值，不过这里并不做未赋值变量之间的检查。</p>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><h4 id="点一致性-一元限制"><a href="#点一致性-一元限制" class="headerlink" title="点一致性(一元限制)"></a>点一致性(一元限制)</h4><p>如果一个点的定义域内所有的值都满足所有的一元限制，那它拥有点一致性</p>
<h4 id="弧一致性-二元限制"><a href="#弧一致性-二元限制" class="headerlink" title="弧一致性(二元限制)"></a>弧一致性(二元限制)</h4><p>两个变量$X\rightarrow Y$具有弧一致性，当且仅当对$X$的每个值都有对应的$Y$的值满足二元条件。</p>
<p><strong>弧一致性检验</strong></p>
<p>AC-3算法，复杂度$O(n^2d^3)$。</p>
<p>$\text{REVISE()}$函数根据给定的两个变量，将第一个变量定义域中找不到第二个变量定义域对应值的值删掉，只有删了至少一个值的时候才会返回真，说明需要对前者的邻居再进行$\text{REVISE}$，复杂度$O(d^2)$，$d$是定义域大小。</p>
<p>$\text{AC-3()}$函数维护一个初始包含所有弧的队列，每次取出一个做$\text{REVISE}$，如果前变量的定义域缩小了，那就用它接着更新它的其他邻居，直到所有弧都出队(被满足)，或是有一个点的定义域被删干净了(不满足)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function AC-3(csp)</span><br><span class="line">returns False if an inconsistency is found, True otherwise</span><br><span class="line">inputs: csp, a binary CSP with components (X, D, C)</span><br><span class="line">local variables: queue, a queue of arcs, initially all the arcs in csp</span><br><span class="line">while queue is not empty do</span><br><span class="line">    (Xi, Xj) = REMOVE-FIRST(queue)</span><br><span class="line">    if REVISE(csp, Xi, Xj)then</span><br><span class="line">        if size of Di = 0 then return False</span><br><span class="line">        for each Xk in Xi.NEIGHBORS- &#123;Xj&#125; do</span><br><span class="line">            add (Xk,Xi) to queue</span><br><span class="line">return true</span><br><span class="line"></span><br><span class="line">function REVISE(csp, Xi, Xj)</span><br><span class="line">returns True if we revise the domain of Xi</span><br><span class="line">revised = False</span><br><span class="line">for each x in Di do</span><br><span class="line">    if no value y in Dj allows (x, y) to satisfy the constraint between Xi and Xj then</span><br><span class="line">        delete x from D;</span><br><span class="line">        revised = True</span><br><span class="line">return revised</span><br></pre></td></tr></table></figure>

<h4 id="路径一致性-多元限制"><a href="#路径一致性-多元限制" class="headerlink" title="路径一致性(多元限制)"></a>路径一致性(多元限制)</h4><p>弧一致性的多元推广。</p>
<h2 id="Chapter-6-逻辑"><a href="#Chapter-6-逻辑" class="headerlink" title="Chapter 6 - 逻辑"></a>Chapter 6 - 逻辑</h2><p><del>经过了数理逻辑导论和离散数学的洗礼</del>我觉得这部分还是得复习一下，至少要知道Resolution和Forward&#x2F;Backward Chaining怎么证。</p>
<h3 id="铺垫"><a href="#铺垫" class="headerlink" title="铺垫"></a>铺垫</h3><p>原子命题，复合命题，逻辑算符，重言式与矛盾式</p>
<p>Inference: Modus Ponens</p>
<p>Common Rules: Addition, Simplification, Disjunctive-syllogism, Hypothetical-syllogism</p>
<p>Entailment, Semantics: KB ⊨ α</p>
<p>Inference, Syntax: KB ⊢ α</p>
<p> Sound &amp; Complete, Validity &amp; Satisfiability </p>
<h3 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h3><p>使用Resolution rule：一堆$\vee$连接的文字中的某个文字的否命题与其同时成立，则可以将该文字剔除。</p>
<p>$$l_1\vee\cdots\vee l_k\quad \overline{l_i}\over l_1\vee\cdots\vee l_{i-1}\vee l_{i+1}\vee\cdots\vee l_k$$</p>
<p>一般转成合取范式(CNF)再用Resolution rule。</p>
<h3 id="Forward-Backward-chaining"><a href="#Forward-Backward-chaining" class="headerlink" title="Forward &#x2F; Backward chaining"></a>Forward &#x2F; Backward chaining</h3><p>在霍恩子句上用Modus Ponens。</p>
<p>前向链接：把已知逻辑式塞到所有规则的前提里，把结论塞回知识库，直到解决问题。对命题逻辑是完备的。</p>
<ul>
<li>数据驱动，自动化，无意识推理</li>
<li>可能做很多无用功</li>
</ul>
<p>后向链接：为了得到问题的结论，需要得到问题的前提，递归证明前提。可以做记忆化。时间上是线性的，对霍恩子句是完备的。</p>
<ul>
<li>目标驱动，适用于解决问题</li>
<li>复杂度会远远小于$O(|\text{Knowledge Base}|)$</li>
</ul>
<p><del>一阶逻辑？ Nobody cares(</del></p>
<h2 id="Chapter-7-机器学习概述"><a href="#Chapter-7-机器学习概述" class="headerlink" title="Chapter 7 - 机器学习概述"></a>Chapter 7 - 机器学习概述</h2><blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p>Tom Mitchell. Machine Learning 1997.</p>
</blockquote>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习：训练样本没有标签</p>
<p>应用：聚类，切割</p>
<p>算法：K-means, Gaussian mixtures, hierarchical clustering, spectral clustering, etc.</p>
<h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>监督学习：训练样本带有对应的标签</p>
<p>应用：分类(样本→离散)，回归(样本→连续)</p>
<p>分类算法：Support Vector Machines, neural networks, decision trees, K-nearest neighbors, naïve Bayes, etc.</p>
<h3 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h3><p>两个相似的样本应该拥有相同的标签。</p>
<p>每个样本视为$d$维空间的点，距离用欧氏距离表示。</p>
<p>训练算法：直接把样本和标签丢到训练集就行</p>
<p>分类算法：给定一个测试样本，看看距离它最近的$k$个样本标签是什么，哪个标签样本数多，测试样本就被分到哪个标签。</p>
<p>优点：</p>
<ul>
<li>简单</li>
<li>实际效果好</li>
<li>不需要建立模型，或者做假设，又或者调参</li>
<li>有新样本很快就扩展了</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要很大的空间来存整个训练集</li>
<li>慢，给定$n$样本$d$特征，需要$O(nd)$才能跑出来</li>
<li>维数灾难：样本分布位于中心附近的概率，随着维度的增加，越来越低；而样本处在边缘的概率，则越来越高。</li>
</ul>
<h3 id="Train-Validation-and-Test"><a href="#Train-Validation-and-Test" class="headerlink" title="Train, Validation and Test"></a>Train, Validation and Test</h3><p>样本内误差：</p>
<p>$$E^{\text{train}}(f) &#x3D; \sum\limits_{i&#x3D;1}^n \text{loss}(y_i,f(x_i))$$</p>
<p>损失函数$\text{loss()}$可以是分类误差(不一样时1)，也可以是均方误差(差值平方和)</p>
<p>目标是最小化训练误差，同时希望样本外误差(测试误差)最小。</p>
<h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><img src="https://s2.loli.net/2023/06/13/DzvuyEiZMP172UW.png" alt="image.png" style="zoom:50%;" />

<p>这幅图蛮重要的，嗯。</p>
<p><strong>如何避免过拟合？</strong></p>
<ul>
<li>减少特征数量：特征选择</li>
<li>模型选择</li>
<li>正则化(regularization)：削弱部分特征的重要程度</li>
<li>做交叉验证(cross-validation)，检验测试误差</li>
</ul>
<p><strong>正则化</strong></p>
<p>最小化：$\sum_{i&#x3D;1}^{n} \operatorname{loss}\left(y_{i}, f\left(x_{i}\right)\right)+C \times R(f)$</p>
<p>目标是避免高阶多项式的出现</p>
<h3 id="Train-Validation-and-Test-1"><a href="#Train-Validation-and-Test-1" class="headerlink" title="Train, Validation and Test"></a>Train, Validation and Test</h3><p>把数据集分成三部分，训练集给样本给标签，验证集给样本看结果用来调参，测试集用来评估最终性能。</p>
<h4 id="K-fold交叉验证"><a href="#K-fold交叉验证" class="headerlink" title="K-fold交叉验证"></a>K-fold交叉验证</h4><p>把数据集分成等大的$k$个子集，每次把其中一个挑出来用来当测试集算误差，最后把所有的误差再加起来。</p>
<h4 id="混淆矩阵-Confusion-matrix"><a href="#混淆矩阵-Confusion-matrix" class="headerlink" title="混淆矩阵(Confusion matrix)"></a>混淆矩阵(Confusion matrix)</h4><table>
<thead>
<tr>
<th></th>
<th>Actual</th>
<th>Label</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Predicted</strong></td>
<td>True Positive</td>
<td>False Positive</td>
</tr>
<tr>
<td><strong>Label</strong></td>
<td>False Negative</td>
<td>True Negative</td>
</tr>
</tbody></table>
<p><strong>Accuracy</strong>: $\text{(TP+TN)&#x2F;Total}$</p>
<p><strong>Precision</strong>:  $\text{TP&#x2F;(TP+FP)}$</p>
<p><strong>Sensitivity</strong>:  $\text{TP&#x2F;(TP+FN)}$</p>
<p><strong>Specificity</strong>:  $\text{TN&#x2F;(TN+FP)}$</p>
<h2 id="Chapter-8-线性回归与对数回归"><a href="#Chapter-8-线性回归与对数回归" class="headerlink" title="Chapter 8 - 线性回归与对数回归"></a>Chapter 8 - 线性回归与对数回归</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>$f(x) &#x3D; \beta_0 + \sum\limits_{j&#x3D;1}^d \beta_jx_j$，其中$\beta_j\in \mathbb R,\ j\in {1,\cdots,d}$</p>
<p>学习线性模型其实就是学习$\beta$。</p>
<p>最小化损失函数$R$：</p>
<p>$$R&#x3D;\frac{1}{2n}\sum\limits_{i&#x3D;1}^n(y_i-f(x_i))^2&#x3D;\left.\frac{1}{2 n} \sum_{i&#x3D;1}^{n}\left(y_{i}-\beta_{0}-\sum_{j&#x3D;1}^{d} \beta_{j} x_{i j}\right)\right)^{2}$$</p>
<p>如果用矩阵形式表示：</p>
<img src="https://s2.loli.net/2023/06/13/LYKqpsglET7nU3W.png" alt="image.png" style="zoom: 50%;" />

<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p>我们有$R(\beta) &#x3D; \frac{1}{2n}||(y-X\beta)||^2 &#x3D; \frac{1}{2n}(y-X\beta)^T(y-X\beta)$，求偏导有</p>
<p>$$\frac{\partial R}{\partial \beta}&#x3D;-\frac{1}{n}X^T(y-X\beta),\ \frac{\partial^2R}{\partial \beta^2}&#x3D;\frac{1}{n}X^TX$$</p>
<p>二阶偏导为正，说明正定，令一阶偏导为0求得$\beta_\min &#x3D; (X^TX)^{-1}X^Ty$</p>
<p>优点：不用定义收敛率，也不用迭代</p>
<p>缺点：需要保证$X^TX$可逆；当维数$d$很高的时候求逆是$O(d^3)$的，很慢。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>同时让$\beta$的每个分量沿着梯度下降：</p>
<p>$\beta_i &#x3D; \beta_i-\alpha \frac{\partial R}{\partial \beta_i}$，其中$\alpha$是学习率。</p>
<p>优点：高维依然很管用</p>
<p>缺点：需要迭代，有时候迭代很多代才收敛；需要调参$\alpha$</p>
<h4 id="实际考虑"><a href="#实际考虑" class="headerlink" title="实际考虑"></a>实际考虑</h4><ul>
<li>一般会把数据做个归一化，把数据变小点</li>
<li>学习率不能太大，也不能太小</li>
<li>损失函数应该随着迭代递减</li>
<li>如果优化程度在$\epsilon$之内说明收敛了</li>
<li>什么时候$X^TX$不可逆？<ul>
<li>样本过少，特征过多</li>
<li>特征之间线性相关</li>
</ul>
</li>
</ul>
<h3 id="对数回归"><a href="#对数回归" class="headerlink" title="对数回归"></a>对数回归</h3><p>做分类，线性回归也不是不行，只是表现一般，而且预测值有时候又不在$[0, 1]$之间。</p>
<p>对数回归不是一个回归方法，而是一个分类方法，虽然怪怪的但是课件原话。</p>
<p>$$g(x) &#x3D; \text{SIGMOID}(z) &#x3D; \frac{e^z}{1+e^z}$$</p>
<img src="https://s2.loli.net/2023/06/13/xFzXuBYUosTMe4w.png" alt="image.png" style="zoom:50%;" />

<p>对数回归用的$f$就不太一样了，相当于是把输出映射到了$[0,1]$</p>
<p>$$f(x)&#x3D;g(\beta_0+\sum\limits_{j&#x3D;1}^d\beta_jx_j),\ R&#x3D;\frac{1}{2n}\sum\limits_{i&#x3D;1}^n(y_i-f(x_i))^2$$</p>
<p>分类的时候，离哪边近就分到哪边。</p>
<p>旧损失函数$\text{Loss}&#x3D;\frac 1 2(f(x)-y)^2$这个时候就不是简单的二次函数了，可能有很多局部最优，所以对旧损失函数的梯度下降寄了。为了全局最优，对损失使用凸函数罢！</p>
<p>$$\text{Cost}(f(x),y)&#x3D;\left{\begin{align*}&amp;-\log(f(x))\quad&amp;\text{if } y &#x3D; 1\&amp;-\log(1-f(x))\quad&amp;\text{if } y &#x3D; 0\end{align*}\right.$$</p>
<p>所以有损失函数：</p>
<p>$$\text{Loss}(f(x),y)&#x3D;-y\log(f(x))-(1-y)\log(1-f(x))$$</p>
<p>$$R(\beta)&#x3D;\frac{1}{n}\sum\limits_{i&#x3D;1}^n\text{Loss}_i$$</p>
<p>好了，我们又可以梯度下降了。</p>
<h2 id="Chapter-9-支持向量机"><a href="#Chapter-9-支持向量机" class="headerlink" title="Chapter 9 - 支持向量机"></a>Chapter 9 - 支持向量机</h2><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p>线性模型的分类靠超平面：$f(x)&#x3D;\left \langle w,x\right \rangle+b&#x3D;0$</p>
<p>支持向量机要求所有点都在正负超平面之外(含超平面)，正负超平面：$\left \langle w,x\right \rangle+b&#x3D;±c$</p>
<p> 不过我们可以通过放缩变成：$\left \langle w,x\right \rangle+b&#x3D;±1$</p>
<p>在正负超平面上的俩决策临界点连线垂直三个超平面，所以有$x^+-x^-&#x3D;\lambda w$</p>
<p>又有$\lambda&#x3D;\frac{2}{||w||_2^2}$，正负超平面之间的距离就是$M&#x3D;\frac 2 {||w||_2}$</p>
<p>现在的目标就是最大化$M$，满足</p>
<p>$$\left \langle w,x_i\right \rangle+b\left{\begin{align*}&amp;\ge 1\quad &amp;\text{if } y_i&amp;&#x3D;1\&amp;\le -1\quad &amp;\text{if } y_i&amp;&#x3D;-1\end{align*}\right.$$</p>
<p>再换句话，就是求$w,b$来达到$\min\limits_w\frac{||w||^2_2}{2}$，条件是$y_i(\left \langle w,x_i\right \rangle+b)\ge 1$</p>
<p><del>过程太复杂不想写了aaaaa</del></p>
<p>等明天醒了补！</p>
<p><strong>解题步骤</strong></p>
<ul>
<li>解对偶优化问题，算出$\alpha^\star$</li>
<li>用$\alpha^\star$算出$w(\alpha^\star)$和$b(\alpha^\star)$</li>
<li>用这个式子分类：$\text{sign}(\sum\limits_{i&#x3D;1}^n\alpha_i^\star y_ik(x_i,x)+b(\alpha^\star))$</li>
</ul>
<h3 id="非线性模型"><a href="#非线性模型" class="headerlink" title="非线性模型"></a>非线性模型</h3><p>把输入空间映射到特征空间，特征空间维数越高，数据越可能线性可分。</p>
<p>映射完之后该咋做咋做就好了，映射用的函数叫核函数。</p>
<p>线性核，多项式核，高斯核</p>
<h2 id="Chapter-10-感知机与神经网络"><a href="#Chapter-10-感知机与神经网络" class="headerlink" title="Chapter 10 - 感知机与神经网络"></a>Chapter 10 - 感知机与神经网络</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>$f(x_i)&#x3D;\text{sign}(\sum\limits_{j&#x3D;0}^dw_jx_{ij})$</p>
<p>如果数据线性可分那就好办，否则不收敛</p>
<p>从一个随机的超平面开始，用训练数据来调整的迭代方法</p>
<p>发现$y_if(x_i)\le 0$时，更新全部$w_j&#x3D;w_j+y_ix_{ij}$</p>
<p>感性理解，就是把每个分量拉回来对应的$x$值</p>
<p>$w_j$决定了$x_j$对结果的权重，可能得到很多解</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>感知机用的是分段函数，换成SIGMOID，也可以换成别的激活函数比如tanh</p>
<p>简单的例子：用OR，NAND，AND感知机拼成XOR网络</p>
<h4 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h4><p>明天醒的时候再看</p>
<h2 id="Chapter-11-决策树和朴素贝叶斯"><a href="#Chapter-11-决策树和朴素贝叶斯" class="headerlink" title="Chapter 11 - 决策树和朴素贝叶斯"></a>Chapter 11 - 决策树和朴素贝叶斯</h2><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>还是做分类的。不需要实数输入，所以不需要数值化特征。</p>
<ol>
<li>主要的决策：下一个用哪个标准分类？</li>
<li>用同质性作为衡量标准：如果分类两边样本数量相等同质性最高，如果分完只剩一类了那就最低。</li>
<li>熵函数 $\text{Entropy}(S)&#x3D;-p_+\log p_+ -p_-\log p_-$是个关于x&#x3D;0.5对称的凸函数</li>
</ol>
<p>收获函数$\text{Gain}(S,A)&#x3D;\text{Entropy}(S)-\sum\limits_{v\in\text{Value(A)}}\frac{|S_v|}{|S|}\text{Entropy}(S_v)$</p>
<p>把子结点人数占比作为权重，子结点熵值作为权值，父节点熵值减去子结点熵值的加权平均就是熵值收获。</p>
<p>很容易过拟合，两种解法：要么别长出来，要么先长出来再剪(这个更好，用一个验证集检验剪枝后有没有比之前更差)。</p>
<p><strong>CART</strong>用$Gini&#x3D;1-p_+^2-p_-^2$代替信息熵。</p>
<p><strong>实际问题</strong></p>
<ul>
<li>先降维，保留特征性最强的特征</li>
<li>用集成方法：随机森林</li>
<li>先均衡数据集：对主要数据欠采样，对少数数据过采样</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>简单可解释</li>
<li>可以转成分类规则</li>
<li>对于分类数据很好</li>
<li>构造简单</li>
<li>不用归一化数据</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>不稳定，一个样本不同树可能就不同</li>
<li>单变量，不处理组合特征</li>
<li>某些结点的选择可能取决于之前的选择</li>
<li>需要平衡数据</li>
</ul>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>$$p(A|B)&#x3D;\frac{p(B|A)*p(A)}{p(B)}&#x3D;\frac{p(B|A)*p(A)}{p(B|\overline A)p(\overline A)+p(B|A)*p(A)}$$</p>
<p>希望通过$p(y|x)$来找到$f(x)&#x3D;y$的映射。</p>
<h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><p>对$p(y|x)$建模，然后给条决策边界来分类。</p>
<h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><p>对$p(x|y)$和$p(y)$建模，然后用贝叶斯算使得$p(y|x)&#x3D;\frac{p(x|y)p(y)}{p(x)}≈p(x|y)p(y)$最大的$y$值作为结果。</p>
<p>输入$x$维数比较高的时候，可以默认不同维特征之间相互独立。</p>
<p><strong>解题步骤</strong></p>
<ul>
<li>对每个标签$y$算$p(y)$</li>
<li>对每个标签$y$和每个特征$a_i$算$p(a_i|y)$</li>
</ul>
<p>$m-$估计概率：$p(a_j|y)&#x3D;\frac{n_c+m*p}{n_y+m}$</p>
<p>其中$p$是事先约定的值，不知道怎么取可以用特征$y$可能的取值数量的倒数</p>
<h2 id="Chapter-12-集成学习与聚类"><a href="#Chapter-12-集成学习与聚类" class="headerlink" title="Chapter 12 - 集成学习与聚类"></a>Chapter 12 - 集成学习与聚类</h2><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>用同样的训练数据，训练独立的弱学习者，策略有三：</p>
<ul>
<li>Boosting</li>
<li>Bagging</li>
<li>Random Forests</li>
</ul>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p>在带权重的训练数据上训练弱学习者，表现好的学习者给更多权重</p>
<p>难样本给更高权重，简单样本给更低权重</p>
<p>错误率决定了最终的贡献比：</p>
<p>$$\alpha_m&#x3D;\frac 1 2 \log(\frac{1-err_m}{err_m})$$</p>
<img src="https://s2.loli.net/2023/06/14/tpjgqJIMOQrNUAb.png" alt="image.png" style="zoom: 33%;" />

<p>数据集上的权重从均权开始，错的人越多权值变得越大，越少就变小</p>
<p>$$w_i&#x3D;w_i*\exp[-\alpha_my_iG_m(x_i)]$$</p>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>Bootstrapping是重采样，按照经验分布进行采样</p>
<p>Bagging和Boosting都基于Bootstrapping</p>
<p>都使用了重采样来生成弱分类器</p>
<p>Bagging &#x3D; Bootstrap aggregation</p>
<p>每次就选等大的一部分数据用来训练一个弱分类器。</p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>不拆数据了，拆特征维数，每次只用一部分特征来分类，多长几棵树投票。</p>
<p>一般拆出来训练的特征维数$m\le\sqrt d$</p>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>无监督学习，分成$k$个聚类</p>
<h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><p>最小化聚类内部的均方差：</p>
<p>$$J&#x3D;\sum\limits_{j&#x3D;1}^k\sum\limits_{x_i\in C_j}||x_i-\mu_j||^2$$</p>
<p>$\mu_j$是第$j$个聚类中的平均值</p>
<p>优点：容易实现</p>
<p>缺点：得知道$K$；维数灾难；没有理论基础</p>
<h4 id="K-means的一些问题"><a href="#K-means的一些问题" class="headerlink" title="K-means的一些问题"></a>K-means的一些问题</h4><p><strong>怎么取$k$？</strong></p>
<p>多试几个，选”elbow-point”</p>
<p>G-means算法</p>
<ol>
<li>用一个小$k$开始</li>
<li>从$k$个聚类中心点跑K-means</li>
<li>看看聚类是否环绕这些中心正态分布</li>
<li>如果OK就不管，如果不是那就拆成两个聚类中心</li>
<li>继续迭代，直到大家都正态分布</li>
</ol>
<p><strong>怎么评价模型？</strong></p>
<p>内部评价：高聚类内部相似度，低聚类外部相似度数据检验，可以用 Davies-Bouldin index评估聚类的紧密型</p>
<p>外部评价：利用对外部数据的知识库做评估(是否符合客观规律)</p>
<p><strong>如果不想要圆形聚类呢？</strong></p>
<p>其他方法：spectral clustering, kernelized K-means, DBSCAN, BIRCH, etc.</p>
<p><strong>怎么初始化？</strong></p>
<p>K-means对聚类中心的选取高度敏感，收敛速度和聚类效果都有影响。</p>
<p>比较安全的技巧是新聚类中心和之前的聚类中心安排得尽量远</p>
<p>解决方案是多重开几次选最好的。</p>
<p><strong>其他的限制？</strong></p>
<p>对一些样本做硬赋值，高斯混合模型允许软赋值(不同的赋值有概率)</p>
<p>对离群的例子高敏感，K-median在这方面的鲁棒性更好</p>
<p>$$\Huge \mathscr{Good\ luck!}$$</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Artificial-Intelligence/" rel="tag"># Artificial Intelligence</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2026/01/11/CS305_Final_Review/" rel="prev" title="CS305 计算机网络 期末复习">
                  <i class="fa fa-angle-left"></i> CS305 计算机网络 期末复习
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2026/01/11/CS323_Final_Review/" rel="next" title="CS323 编译原理 期末复习">
                  CS323 编译原理 期末复习 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">萌ICP备20269202号 </a>
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Gu Tao</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gutaozi.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"slideLeftBigIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="AI这东西随主人的，我学不进去它也学不进去">
<meta property="og:type" content="article">
<meta property="og:title" content="CS311 人工智能 期末复习">
<meta property="og:url" content="https://gutaozi.github.io/2023/06/12/CS311_Final_Review/index.html">
<meta property="og:site_name" content="咕桃w！">
<meta property="og:description" content="AI这东西随主人的，我学不进去它也学不进去">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/TnYsVXi8B56aeuJ.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/DzvuyEiZMP172UW.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/LYKqpsglET7nU3W.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/13/xFzXuBYUosTMe4w.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/14/tpjgqJIMOQrNUAb.png">
<meta property="article:published_time" content="2023-06-12T03:13:10.621Z">
<meta property="article:modified_time" content="2023-06-14T01:10:14.826Z">
<meta property="article:author" content="咕桃">
<meta property="article:tag" content="Artificial Intelligence">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/06/13/TnYsVXi8B56aeuJ.png">

<link rel="canonical" href="https://gutaozi.github.io/2023/06/12/CS311_Final_Review/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>CS311 人工智能 期末复习 | 咕桃w！</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">咕桃w！</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://gutaozi.github.io/2023/06/12/CS311_Final_Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/109007949?v=4">
      <meta itemprop="name" content="咕桃">
      <meta itemprop="description" content="Just Do It, But Not Just Do It.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="咕桃w！">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS311 人工智能 期末复习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-12 11:13:10" itemprop="dateCreated datePublished" datetime="2023-06-12T11:13:10+08:00">2023-06-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-06-14 09:10:14" itemprop="dateModified" datetime="2023-06-14T09:10:14+08:00">2023-06-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
                </span>
            </span>

          
            <div class="post-description">AI这东西随主人的，我学不进去它也学不进去</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>$\huge\text{Outline}$</p>
<ol>
<li>人工智能概述</li>
<li>搜索</li>
<li>启发与元启发</li>
<li>对抗搜索</li>
<li>约束满足问题</li>
<li>逻辑</li>
<li>机器学习概述</li>
<li>线性回归与对数回归</li>
<li>支持向量机</li>
<li>感知机与神经网络</li>
<li>决策树与朴素贝叶斯</li>
<li>集成学习与聚类</li>
<li>强化学习</li>
<li>自然语言处理</li>
</ol>
<hr>
<h2 id="Chapter-1-人工智能概述"><a href="#Chapter-1-人工智能概述" class="headerlink" title="Chapter 1 - 人工智能概述"></a>Chapter 1 - 人工智能概述</h2><p>第一课略，大致是概念的介绍和AI发展的介绍。</p>
<h3 id="智能体"><a href="#智能体" class="headerlink" title="智能体"></a>智能体</h3><p><strong>智能体的特征</strong></p>
<ul>
<li><p>通过探测器感知环境</p>
<p>perceiving its environment through sensors</p>
</li>
<li><p>根据环境利用执行器执行操作</p>
<p>acting upon that environment through actuators</p>
</li>
</ul>
<p>智能体运行周期：感知(perceive)，思考(perceive)，行动(act)</p>
<p>智能体是Architecture和Program的互补兼容。</p>
<p><strong>Rationality的评价标准：PEAS</strong></p>
<ul>
<li>Performance</li>
<li>Environment</li>
<li>Actuators</li>
<li>Sensors</li>
</ul>
<p><strong>智能体的类型</strong></p>
<ul>
<li><p>Simple reflex agents</p>
<p>只根据当前状态决策行动，需要完全可观测环境，且正确行动只与当前观测有关。</p>
</li>
<li><p>Model-based reflex agents</p>
<p>智能机内部状态由历史观测决定，可以处理部分可观测环境</p>
<p>建立环境模型：环境如何独立演变 + 行动如何影响环境</p>
</li>
<li><p>Goal-based agents</p>
<p>拥有目标信息，考虑每种行动对接近目标的贡献</p>
</li>
<li><p>Utility-based agents</p>
<p>Utility function评估智能体性能，选择最大化期望utility的行动</p>
</li>
<li><p>Learning agents</p>
<p>Learning element：根据输入进行提升</p>
<p>Performance element：选择执行的行动</p>
<p>Critic：由固定的性能评估标准决定智能体的表现</p>
<p>Problem Generator：允许智能体进行探索</p>
</li>
</ul>
<p><strong>智能体的状态</strong></p>
<ul>
<li><p>原子表示Atomic Representation</p>
<p>每个环境状态都是不具有内部结构的黑盒</p>
<p>Search, games, Markov decision processes, hidden Markov models, etc</p>
</li>
<li><p>因子表示Factored Representation</p>
<p>每个状态都有一些属性，可以用一组变量来表示</p>
<p> Constraint satisfaction, and Bayesian networks. </p>
</li>
<li><p>结构表示Structured Representation</p>
<p>状态之间的关系可以显式表达</p>
<p>First order logic, knowledge-based learning, natural language understanding. </p>
</li>
</ul>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><strong>环境的类型</strong></p>
<ul>
<li><p>完全可观测Fully observable/部分可观测Partially observable</p>
</li>
<li><p>确定Deterministic/随机Stochastic</p>
<p>下一步的环境是否只由Agent的行动和当前环境决定。</p>
</li>
<li><p>Episodic/Sequential</p>
<p>前者说明每一步行动不影响下一步，后者说明会影响。</p>
</li>
<li><p>静态Static/动态Dynamic</p>
<p>环境在智能体运行时是否变化(半动态：环境不变，但评分变动)</p>
</li>
<li><p>离散Discrete/连续Continuous</p>
</li>
<li><p>单智能体Single-agent/多智能体Multi-agent</p>
</li>
<li><p>已知Known/未知Unknown</p>
<p>设计者是否拥有对环境的知识</p>
</li>
</ul>
<h2 id="Chapter-2-搜索"><a href="#Chapter-2-搜索" class="headerlink" title="Chapter 2 - 搜索"></a>Chapter 2 - 搜索</h2><h3 id="搜索概述"><a href="#搜索概述" class="headerlink" title="搜索概述"></a>搜索概述</h3><p><strong>通过搜索解决问题</strong></p>
<ol>
<li><p>定义问题</p>
<p>(a)目标形式化</p>
<p>(b)问题形式化</p>
</li>
<li><p>两步解决问题</p>
<p>(a)离线搜索不同行动</p>
<p>(b)执行搜索结果</p>
</li>
</ol>
<p><strong>问题形式化</strong></p>
<ul>
<li>初始状态</li>
<li>状态：所有由初始状态，通过任意行动序列可达的状态集合(状态空间)</li>
<li>行动：对于一个状态，智能体可以执行的所有行动(行动空间)</li>
<li>转化模型：描述每个行动对当前状态造成的影响</li>
<li>目标检测：决定给定状态是否已经达到目标</li>
<li>路径开销：根据性能标准为动作序列赋上数值开销的函数</li>
</ul>
<p><strong>状态空间 vs. 搜索空间</strong></p>
<ul>
<li><p>状态空间是实际状态的集合</p>
</li>
<li><p>搜索空间是抽象成搜索树/图的可行解集合</p>
</li>
<li><p>搜索树描述了行动序列</p>
<ul>
<li><p>根节点：初始状态</p>
</li>
<li><p>分支：行动</p>
</li>
<li><p>结点：行动的结果</p>
<p>每个结点有：父节点，子节点，深度，路径开销，在状态空间对应的状态</p>
</li>
</ul>
</li>
<li><p>扩张Expand：对于给定结点，创建所有子结点的函数</p>
</li>
</ul>
<p><strong>搜索空间的三个区域</strong></p>
<ol>
<li>Explored (a.k.a. Closed List, Visited Set) </li>
<li>Frontier  (a.k.a. Ready list, Open List, the Fringe) </li>
<li>Unexplored</li>
</ol>
<p>搜索就是从3到2再到1的过程，搜索策略决定了顺序。</p>
<p><strong>搜索策略</strong></p>
<p>搜索策略由选定结点进行扩张的顺序来定义，评价标准：</p>
<ul>
<li>完备性Completeness</li>
<li>时间复杂度Time complexity</li>
<li>空间复杂度Space complexity</li>
<li>最优性Optimality</li>
</ul>
<p>时间复杂度和空间复杂度的评估：</p>
<ul>
<li>$b$ - 搜索树最大的分支数量</li>
<li>$d$ - 解的深度</li>
<li>$m$ - 状态空间的最大深度</li>
</ul>
<h3 id="无信息搜索"><a href="#无信息搜索" class="headerlink" title="无信息搜索"></a>无信息搜索</h3><p>不使用domain knowledge的搜索。</p>
<ul>
<li><p>Breadth-first search / 广度优先 / BFS</p>
<p>优先探索最浅的结点</p>
</li>
<li><p>Depth-first search / 深度优先 / DFS</p>
<p>优先探索最深的结点</p>
</li>
<li><p>Depth-limited search / 深度限制 / DLS</p>
<p>带深度限制的DFS</p>
</li>
<li><p>Iterative-deepening search / 迭代加深 / IDS</p>
<p>不断放宽深度限制(提高深度上限)的DLS</p>
</li>
<li><p>Uniform-cost search (UCS) / 统一代价 / UCS</p>
<p>优先探索开销最小的结点（参考Dijkstra）</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>完备性</th>
<th>时间</th>
<th>空间</th>
<th>最优性</th>
<th>实现</th>
</tr>
</thead>
<tbody>
<tr>
<td>BFS</td>
<td>有限分支则完备</td>
<td>$O(b^d)$</td>
<td>$O(b^d)$</td>
<td>每步开销都为1则最优</td>
<td>Queue</td>
</tr>
<tr>
<td>DFS</td>
<td>有限状态空间则完备</td>
<td>$O(b^m)$</td>
<td>$O(bm)$</td>
<td>否</td>
<td>Stack</td>
</tr>
<tr>
<td>UCS</td>
<td>有限开销则完备</td>
<td>$O(b^{C^*/\epsilon})$</td>
<td>$O(b^{C^*/\epsilon})$</td>
<td>是</td>
<td>Heap</td>
</tr>
</tbody>
</table>
</div>
<p>* 其中$C^*$是最优解的开销，每一步行动至少开销$\epsilon$</p>
<h3 id="有信息搜索"><a href="#有信息搜索" class="headerlink" title="有信息搜索"></a>有信息搜索</h3><p>使用领域知识来获取关于当前状态与目标距离的信息。</p>
<h4 id="贪心BFS"><a href="#贪心BFS" class="headerlink" title="贪心BFS"></a>贪心BFS</h4><p>把到目标的估计费用$h(n)$当作唯一标准贪心搜索</p>
<h4 id="A-搜索"><a href="#A-搜索" class="headerlink" title="A*搜索"></a>A*搜索</h4><p>$f(n)=$到当前结点的费用$g(n)$+当前结点到目标的估计费用$h(n)$</p>
<p>$f(n)$是经过结点$n$的最优路径费用的估值</p>
<p>完备性：是</p>
<p>时间复杂度：指数级</p>
<p>空间复杂度：每个结点都在内存里，最大的问题</p>
<p>最优性：是</p>
<h4 id="可接受的启发函数-Admissible-heuristics"><a href="#可接受的启发函数-Admissible-heuristics" class="headerlink" title="可接受的启发函数(Admissible heuristics)"></a>可接受的启发函数(Admissible heuristics)</h4><p>如果启发式函数从不高估到达目标的费用，那么就是可接受的(admissible)，并且使用该启发式函数的A*得到的解是最优的。</p>
<p>证明：</p>
<ul>
<li><p>Suppose $G_o$ is the optimal goal.</p>
<p>Suppose $G_s$ is some suboptimal goal.</p>
<p>Suppose $n$ is on the shortest path to $G_o$. </p>
</li>
<li><p>$f(G_s) = g(G_s)$ since $h(G_s) = 0$</p>
<p>$f(G_o ) = g(G_o )$ since $h(G_o ) = 0$</p>
<p>$g(G_s)&gt;g(G_o )$ since $G_s$ is suboptimal</p>
</li>
<li><p>Then $f(G_s) &gt; f(G_o ) . . . (1)$</p>
<p>$h(n)\le h^*(n)$ since h is admissible</p>
<p>$g(n)+h(n)\le g(n)+h^* (n) = g(G_o ) = f(G_o )$</p>
<p>Then $f(n) \le f(G_o ) . . . (2)$</p>
</li>
<li><p>From (1) and (2) $f(G_s)&gt;f(n)$, so A* will never select $G_s$ during the search and hence A* is optimal.</p>
</li>
</ul>
<h4 id="IDA-搜索"><a href="#IDA-搜索" class="headerlink" title="IDA*搜索"></a>IDA*搜索</h4><p>把迭代加深方法加到A*搜索上。</p>
<h3 id="局部搜索"><a href="#局部搜索" class="headerlink" title="局部搜索"></a>局部搜索</h3><p>不关心到目标的路径，优化问题可以使用局部搜索。</p>
<p>只维护当前状态，并通过移动到相邻状态，试图优化。</p>
<ul>
<li>不需要维护搜索树</li>
<li>内存占用小</li>
<li>在连续或较大的状态空间中找到较好解</li>
</ul>
<h4 id="爬山算法"><a href="#爬山算法" class="headerlink" title="爬山算法"></a>爬山算法</h4><p>贪心局部搜索，只选择最优的邻居进行探索，到达峰值时停止(可能是局部峰值)</p>
<p>每个结点代表一个状态和一个值</p>
<p>变种：</p>
<ul>
<li>Sideways moves 允许转移到与当前状态同等优的解</li>
<li>Random-restart 多试几次取最优</li>
<li>Stochastic 随机选择不同的上坡算子</li>
</ul>
<p>改进：</p>
<ul>
<li>Hill climbing：取决于地形，多重开几次还是不错的</li>
<li>Local beam search：同时维护前k优的解，探索其邻居</li>
<li>Stochastic beam search：随机选择k个解(不一定是前k优)，探索其邻居</li>
</ul>
<h4 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h4><p>Stochastic beam search的变种。</p>
<p>自然选择，种群，个体，基因，健壮函数，交叉互换，后代，变异</p>
<h4 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h4><p>温度指数级下降，以一定概率接受较劣解，温度越高跳出当前解的可能性越高。</p>
<p>接受概率：$P = e^{-\Delta E/T}$</p>
<h2 id="Chapter-3-启发与元启发"><a href="#Chapter-3-启发与元启发" class="headerlink" title="Chapter 3 - 启发与元启发"></a>Chapter 3 - 启发与元启发</h2><h3 id="启发式函数进阶"><a href="#启发式函数进阶" class="headerlink" title="启发式函数进阶"></a>启发式函数进阶</h3><h4 id="启发式函数与效率"><a href="#启发式函数与效率" class="headerlink" title="启发式函数与效率"></a>启发式函数与效率</h4><p>如果一个启发式函数始终不小于另一个启发式函数，称其为dominate，前者拥有更好的效率。好的启发式函数可以降低$b^\star$，分支数量化了启发式搜索的效率。</p>
<p>对于一堆admissible的启发式函数$\{h_i\}$，能够dominate所有函数的是$\max\{h_i\}$。</p>
<h4 id="好的启发式函数上哪找？"><a href="#好的启发式函数上哪找？" class="headerlink" title="好的启发式函数上哪找？"></a>好的启发式函数上哪找？</h4><ol>
<li>从放宽条件的问题入手，例如8-puzzle允许块之间重叠，得到的步数总是要比原问题少的。</li>
<li>从子问题入手，例如8-puzzle只把一部分块归位而不管其他的。该问题中，函数$h_{\text{sub}}$(只考虑四块的启发式函数极大值并集)是dominate函数$h_{1step}$的。</li>
<li>根据经验生成，可以选定当前状态的几个特征作为变量，用神经网络/决策树来学这些变量对应的权重，用线性函数表示启发式函数</li>
</ol>
<h3 id="元启发式函数进阶"><a href="#元启发式函数进阶" class="headerlink" title="元启发式函数进阶"></a>元启发式函数进阶</h3><p>与问题相互独立的启发式方法，可以组合使用。</p>
<blockquote>
<p>One general law, leading to the advancement of all organic beings, namely, multiply, vary, let the strongest live and weakest die.</p>
<p>Charles Darwin, The Origin of Species</p>
</blockquote>
<p>演化计算：繁殖，变异，选择</p>
<h2 id="Chapter-4-对抗搜索"><a href="#Chapter-4-对抗搜索" class="headerlink" title="Chapter 4 - 对抗搜索"></a>Chapter 4 - 对抗搜索</h2><h3 id="Minimax搜索"><a href="#Minimax搜索" class="headerlink" title="Minimax搜索"></a>Minimax搜索</h3><p>其中某一方，按照双方都足够聪明的情况进行DFS(DLS)，从搜索树所有分支中选择对各自最优的。</p>
<p>在可以搜索到终局的情况下，对于一个状态$s$，minimax函数：</p>
<script type="math/tex; mode=display">f(s)=\left\{\begin{align*}&\text{Utility(s)}&\text{if Terminal-test(s)}\\ &\max_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&\text{if Player(s)=MAX}\\&\min_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&\text{if Player(s)=MIN}\end{align*}\right.</script><p>然而博弈是有时限的，指数级时间复杂度不可能搜完，所以：</p>
<ul>
<li>将终局评估改成非终局评估</li>
<li>使用迭代加深搜索IDS</li>
<li>剪枝</li>
</ul>
<h3 id="alpha-beta-剪枝"><a href="#alpha-beta-剪枝" class="headerlink" title="$\alpha-\beta$ 剪枝"></a>$\alpha-\beta$ 剪枝</h3><p>$\alpha$：$\text{MAX}$结点目前已经探索的结点中最大的一个。当前结点不会探索比$\alpha$估值还低的结点。</p>
<p>$\beta$：$\text{MIN}$结点目前已经探索的节点中最小的一个。当前结点不会探索比$\beta$估值还高的结点。</p>
<h4 id="什么时候要剪呢？"><a href="#什么时候要剪呢？" class="headerlink" title="什么时候要剪呢？"></a>什么时候要剪呢？</h4><p>$\text{MAX}$视角：$\text{MIN}$在再下一步的结点中，发现了比$\alpha$还低的值，那$\text{Min}$肯定会选这个分支，导致$\text{MAX}$的估值小于$\alpha$，为了避免这种局面，当下一层的$\text{MIN}$发现了一个比$\alpha$还小的值时，说明对手足够聪明的情况下，$\text{MAX}$在这一分支会被对手限制到不如$\alpha$的局面，剪掉。</p>
<p>$\text{MIN}$视角：$\text{MAX}$在再下一步的结点中，发现了比$\beta$还大的值，那$\text{MAX}$肯定会选这个分支，导致$\text{MIN}$的估值大于$\beta$，为了避免这种局面，当下一层的$\text{MAX}$发现了一个比$\beta$还大的值时，说明对手足够聪明的情况下，会选择这个新的分支规避$\beta$的限制，故剪掉。</p>
<h4 id="“自裁式”实现"><a href="#“自裁式”实现" class="headerlink" title="“自裁式”实现"></a>“自裁式”实现</h4><p>上面的说法更像是当前结点主动剪子结点。但还有一种“自裁式”的剪法，就是某个结点发现自己太好了，肯定会被足够聪明的上一层剪掉，就自裁了。</p>
<p>搜索的时候，父结点把自己的$\alpha-\beta$给子结点，探索完一个子结点后，做如下操作：</p>
<p>当前节点是$\text{MAX}$，如果$\text{MIN}$子结点给出的估值比当前最高的还高，那就检查有没有超过当前节点的$\beta$，如果超过了当前节点的$\beta$，说明当前这一支太好了，会被更高层的$\text{MIN}$掐掉，所以这个$\text{MAX}$结点就被整个剪掉了；如果没超过$\beta$而且还比当前最高还高，那就更新当前的最高值和$\alpha$。</p>
<p>当前节点是$\text{MIN}$时同理，如果这个$\text{MIN}$结点能把最优解掐到不足$\alpha$，那$\text{MIN}$结点的$\text{MAX}$父节点肯定不乐意啊，所以一整个$\text{MIN}$结点就被剪掉了。</p>
<p><img src="https://s2.loli.net/2023/06/13/TnYsVXi8B56aeuJ.png" alt="image.png"></p>
<center>Minimax搜索伪代码</center>

<h4 id="换序剪枝"><a href="#换序剪枝" class="headerlink" title="换序剪枝"></a>换序剪枝</h4><p>$\alpha-\beta$ 剪枝的效果受访问顺序影响，有可能导致剪枝的结点最后才被访问，这就剪了个寂寞。</p>
<p>最差的顺序相当于没有剪枝，复杂度是$O(b^m)$。</p>
<p>最好的顺序则是把最“聪明”的结点都放在最先访问，实际复杂度是$O(b^{m/2})$，直接开了个根号。</p>
<p>如何找到一个好的顺序呢？除了动用领域知识，还可以保存历史表，以前发现最好的几步可能在之后还是最好的几步，或是保存状态以供重复时使用。</p>
<h4 id="实际情况"><a href="#实际情况" class="headerlink" title="实际情况"></a>实际情况</h4><p>上面的$\alpha-\beta$剪枝伪代码还是要一直搜到叶子结点，依然是不现实的，所以将$\text{Util(s)}$函数更换为可以评价中局的启发式函数$\text{eval(s)}$。</p>
<h3 id="期望Minimax搜索"><a href="#期望Minimax搜索" class="headerlink" title="期望Minimax搜索"></a>期望Minimax搜索</h3><p>对于带有随机性的博弈，期望Minimax搜索在$\text{MIN}$和$\text{MAX}$之外又加入了$\text{Chance}$层，会返回当前所有可能发生的情况合起来的期望。</p>
<script type="math/tex; mode=display">f(s)=\left\{\begin{align*}&\text{Utility(s)}&\text{if Terminal-test(s)}\\ &\max_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&\text{if Player(s)=MAX}\\&\min_{a\in\text{Actions(s)}}f(\text{Result(s,a)})&\text{if Player(s)=MIN}\\&\sum\limits_{r}P(r)f(\text{Result(s,r)})&\text{if Player(s)=Chance}\end{align*}\right.</script><p>其中$r$代表了所有的随机事件。</p>
<h2 id="Chapter-5-约束满足问题"><a href="#Chapter-5-约束满足问题" class="headerlink" title="Chapter 5 - 约束满足问题"></a>Chapter 5 - 约束满足问题</h2><h3 id="CSP概述"><a href="#CSP概述" class="headerlink" title="CSP概述"></a>CSP概述</h3><p>也是搜索问题，但是我们更关心目标本身。</p>
<p>CSP的状态是因子表示的，也就是由一组变量表示。</p>
<p>一个目标测试(goal test)是对于这组变量(的子集)的一组限制。</p>
<p><strong>CSP问题的三个要素</strong></p>
<ol>
<li>变量集合$X$</li>
<li>变量定义域的集合$D$</li>
<li>约束的集合$C$</li>
</ol>
<p>CSP的解：在定义域$D$中，对变量$X$找到一个合适的赋值方案$s$，满足所有的约束$C$。这样的解称为一致的赋值(consistent assignment)。</p>
<p>变量类型：离散/连续</p>
<p>限制类型：一元(unary)，二元(binary)，全局(global)，偏好/软限制(preferences/soft constraints)</p>
<h3 id="解决CSP"><a href="#解决CSP" class="headerlink" title="解决CSP"></a>解决CSP</h3><p>暴力太慢，配合推理(inference)和搜索(search)来加速。</p>
<p>Inference主要是约束传播(constraint propagation)，是指一个变量的合法取值变少时，其他变量的合法取值也会变少。</p>
<p>Search的DFS和BFS都是老朋友了，新来了个BTS。</p>
<p>回溯搜索(Backtracking search, BTS)也是一种DFS，满足以下条件：</p>
<ul>
<li>一次赋一个变量的值：赋值顺序是可交换的</li>
<li>赋值的时候就注意和之前的赋值不能冲突</li>
</ul>
<p><del>感觉和DFS没区别啊= =</del></p>
<h3 id="改进BTS的三种方法"><a href="#改进BTS的三种方法" class="headerlink" title="改进BTS的三种方法"></a>改进BTS的三种方法</h3><h4 id="最小剩余值-Minimum-Remaining-Value"><a href="#最小剩余值-Minimum-Remaining-Value" class="headerlink" title="最小剩余值(Minimum Remaining Value)"></a>最小剩余值(Minimum Remaining Value)</h4><p>Q：先给哪个变量赋值？</p>
<blockquote>
<p>先挑硬柿子捏</p>
</blockquote>
<p>每次挑剩余选择最少的结点赋值。</p>
<h4 id="最小限制值-Least-Constraining-Value"><a href="#最小限制值-Least-Constraining-Value" class="headerlink" title="最小限制值(Least Constraining Value)"></a>最小限制值(Least Constraining Value)</h4><p>Q：给这个变量赋哪个值？</p>
<blockquote>
<p>少给后人添麻烦</p>
</blockquote>
<p>每次挑对别人限制最少的结点赋值。</p>
<h4 id="前向检查-Forward-Checking"><a href="#前向检查-Forward-Checking" class="headerlink" title="前向检查(Forward Checking)"></a>前向检查(Forward Checking)</h4><p>Q：世界末日是可预见的吗？</p>
<blockquote>
<p>关心每个人的未来</p>
</blockquote>
<p>每次赋值后检查所有未赋值变量定义域是否为空。</p>
<p>在前向检查的时候用到了约束传播的思想，用已赋值的变量去限制未赋值的变量，看还有没有合法赋值，不过这里并不做未赋值变量之间的检查。</p>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><h4 id="点一致性-一元限制"><a href="#点一致性-一元限制" class="headerlink" title="点一致性(一元限制)"></a>点一致性(一元限制)</h4><p>如果一个点的定义域内所有的值都满足所有的一元限制，那它拥有点一致性</p>
<h4 id="弧一致性-二元限制"><a href="#弧一致性-二元限制" class="headerlink" title="弧一致性(二元限制)"></a>弧一致性(二元限制)</h4><p>两个变量$X\rightarrow Y$具有弧一致性，当且仅当对$X$的每个值都有对应的$Y$的值满足二元条件。</p>
<p><strong>弧一致性检验</strong></p>
<p>AC-3算法，复杂度$O(n^2d^3)$。</p>
<p>$\text{REVISE()}$函数根据给定的两个变量，将第一个变量定义域中找不到第二个变量定义域对应值的值删掉，只有删了至少一个值的时候才会返回真，说明需要对前者的邻居再进行$\text{REVISE}$，复杂度$O(d^2)$，$d$是定义域大小。</p>
<p>$\text{AC-3()}$函数维护一个初始包含所有弧的队列，每次取出一个做$\text{REVISE}$，如果前变量的定义域缩小了，那就用它接着更新它的其他邻居，直到所有弧都出队(被满足)，或是有一个点的定义域被删干净了(不满足)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function AC-3(csp)</span><br><span class="line">returns False if an inconsistency is found, True otherwise</span><br><span class="line">inputs: csp, a binary CSP with components (X, D, C)</span><br><span class="line">local variables: queue, a queue of arcs, initially all the arcs in csp</span><br><span class="line">while queue is not empty do</span><br><span class="line">    (Xi, Xj) = REMOVE-FIRST(queue)</span><br><span class="line">    if REVISE(csp, Xi, Xj)then</span><br><span class="line">        if size of Di = 0 then return False</span><br><span class="line">        for each Xk in Xi.NEIGHBORS- &#123;Xj&#125; do</span><br><span class="line">            add (Xk,Xi) to queue</span><br><span class="line">return true</span><br><span class="line"></span><br><span class="line">function REVISE(csp, Xi, Xj)</span><br><span class="line">returns True if we revise the domain of Xi</span><br><span class="line">revised = False</span><br><span class="line">for each x in Di do</span><br><span class="line">    if no value y in Dj allows (x, y) to satisfy the constraint between Xi and Xj then</span><br><span class="line">        delete x from D;</span><br><span class="line">        revised = True</span><br><span class="line">return revised</span><br></pre></td></tr></table></figure>
<h4 id="路径一致性-多元限制"><a href="#路径一致性-多元限制" class="headerlink" title="路径一致性(多元限制)"></a>路径一致性(多元限制)</h4><p>弧一致性的多元推广。</p>
<h2 id="Chapter-6-逻辑"><a href="#Chapter-6-逻辑" class="headerlink" title="Chapter 6 - 逻辑"></a>Chapter 6 - 逻辑</h2><p><del>经过了数理逻辑导论和离散数学的洗礼</del>我觉得这部分还是得复习一下，至少要知道Resolution和Forward/Backward Chaining怎么证。</p>
<h3 id="铺垫"><a href="#铺垫" class="headerlink" title="铺垫"></a>铺垫</h3><p>原子命题，复合命题，逻辑算符，重言式与矛盾式</p>
<p>Inference: Modus Ponens</p>
<p>Common Rules: Addition, Simplification, Disjunctive-syllogism, Hypothetical-syllogism</p>
<p>Entailment, Semantics: KB ⊨ α</p>
<p>Inference, Syntax: KB ⊢ α</p>
<p> Sound &amp; Complete, Validity &amp; Satisfiability </p>
<h3 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h3><p>使用Resolution rule：一堆$\vee$连接的文字中的某个文字的否命题与其同时成立，则可以将该文字剔除。</p>
<script type="math/tex; mode=display">l_1\vee\cdots\vee l_k\quad \overline{l_i}\over l_1\vee\cdots\vee l_{i-1}\vee l_{i+1}\vee\cdots\vee l_k</script><p>一般转成合取范式(CNF)再用Resolution rule。</p>
<h3 id="Forward-Backward-chaining"><a href="#Forward-Backward-chaining" class="headerlink" title="Forward / Backward chaining"></a>Forward / Backward chaining</h3><p>在霍恩子句上用Modus Ponens。</p>
<p>前向链接：把已知逻辑式塞到所有规则的前提里，把结论塞回知识库，直到解决问题。对命题逻辑是完备的。</p>
<ul>
<li>数据驱动，自动化，无意识推理</li>
<li>可能做很多无用功</li>
</ul>
<p>后向链接：为了得到问题的结论，需要得到问题的前提，递归证明前提。可以做记忆化。时间上是线性的，对霍恩子句是完备的。</p>
<ul>
<li>目标驱动，适用于解决问题</li>
<li>复杂度会远远小于$O(|\text{Knowledge Base}|)$</li>
</ul>
<p><del>一阶逻辑？ Nobody cares(</del></p>
<h2 id="Chapter-7-机器学习概述"><a href="#Chapter-7-机器学习概述" class="headerlink" title="Chapter 7 - 机器学习概述"></a>Chapter 7 - 机器学习概述</h2><blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p>Tom Mitchell. Machine Learning 1997.</p>
</blockquote>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习：训练样本没有标签</p>
<p>应用：聚类，切割</p>
<p>算法：K-means, Gaussian mixtures, hierarchical clustering, spectral clustering, etc.</p>
<h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>监督学习：训练样本带有对应的标签</p>
<p>应用：分类(样本→离散)，回归(样本→连续)</p>
<p>分类算法：Support Vector Machines, neural networks, decision trees, K-nearest neighbors, naïve Bayes, etc.</p>
<h3 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h3><p>两个相似的样本应该拥有相同的标签。</p>
<p>每个样本视为$d$维空间的点，距离用欧氏距离表示。</p>
<p>训练算法：直接把样本和标签丢到训练集就行</p>
<p>分类算法：给定一个测试样本，看看距离它最近的$k$个样本标签是什么，哪个标签样本数多，测试样本就被分到哪个标签。</p>
<p>优点：</p>
<ul>
<li>简单</li>
<li>实际效果好</li>
<li>不需要建立模型，或者做假设，又或者调参</li>
<li>有新样本很快就扩展了</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要很大的空间来存整个训练集</li>
<li>慢，给定$n$样本$d$特征，需要$O(nd)$才能跑出来</li>
<li>维数灾难：样本分布位于中心附近的概率，随着维度的增加，越来越低；而样本处在边缘的概率，则越来越高。</li>
</ul>
<h3 id="Train-Validation-and-Test"><a href="#Train-Validation-and-Test" class="headerlink" title="Train, Validation and Test"></a>Train, Validation and Test</h3><p>样本内误差：</p>
<script type="math/tex; mode=display">E^{\text{train}}(f) = \sum\limits_{i=1}^n \text{loss}(y_i,f(x_i))</script><p>损失函数$\text{loss()}$可以是分类误差(不一样时1)，也可以是均方误差(差值平方和)</p>
<p>目标是最小化训练误差，同时希望样本外误差(测试误差)最小。</p>
<h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p><img src="https://s2.loli.net/2023/06/13/DzvuyEiZMP172UW.png" alt="image.png" style="zoom:50%;" /></p>
<p>这幅图蛮重要的，嗯。</p>
<p><strong>如何避免过拟合？</strong></p>
<ul>
<li>减少特征数量：特征选择</li>
<li>模型选择</li>
<li>正则化(regularization)：削弱部分特征的重要程度</li>
<li>做交叉验证(cross-validation)，检验测试误差</li>
</ul>
<p><strong>正则化</strong></p>
<p>最小化：$\sum_{i=1}^{n} \operatorname{loss}\left(y_{i}, f\left(x_{i}\right)\right)+C \times R(f)$</p>
<p>目标是避免高阶多项式的出现</p>
<h3 id="Train-Validation-and-Test-1"><a href="#Train-Validation-and-Test-1" class="headerlink" title="Train, Validation and Test"></a>Train, Validation and Test</h3><p>把数据集分成三部分，训练集给样本给标签，验证集给样本看结果用来调参，测试集用来评估最终性能。</p>
<h4 id="K-fold交叉验证"><a href="#K-fold交叉验证" class="headerlink" title="K-fold交叉验证"></a>K-fold交叉验证</h4><p>把数据集分成等大的$k$个子集，每次把其中一个挑出来用来当测试集算误差，最后把所有的误差再加起来。</p>
<h4 id="混淆矩阵-Confusion-matrix"><a href="#混淆矩阵-Confusion-matrix" class="headerlink" title="混淆矩阵(Confusion matrix)"></a>混淆矩阵(Confusion matrix)</h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Actual</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Predicted</strong></td>
<td>True Positive</td>
<td>False Positive</td>
</tr>
<tr>
<td><strong>Label</strong></td>
<td>False Negative</td>
<td>True Negative</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Accuracy</strong>: $\text{(TP+TN)/Total}$</p>
<p><strong>Precision</strong>:  $\text{TP/(TP+FP)}$</p>
<p><strong>Sensitivity</strong>:  $\text{TP/(TP+FN)}$</p>
<p><strong>Specificity</strong>:  $\text{TN/(TN+FP)}$</p>
<h2 id="Chapter-8-线性回归与对数回归"><a href="#Chapter-8-线性回归与对数回归" class="headerlink" title="Chapter 8 - 线性回归与对数回归"></a>Chapter 8 - 线性回归与对数回归</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>$f(x) = \beta_0 + \sum\limits_{j=1}^d \beta_jx_j$，其中$\beta_j\in \mathbb R,\ j\in \{1,\cdots,d\}$</p>
<p>学习线性模型其实就是学习$\beta$。</p>
<p>最小化损失函数$R$：</p>
<script type="math/tex; mode=display">R=\frac{1}{2n}\sum\limits_{i=1}^n(y_i-f(x_i))^2=\left.\frac{1}{2 n} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{d} \beta_{j} x_{i j}\right)\right)^{2}</script><p>如果用矩阵形式表示：</p>
<p><img src="https://s2.loli.net/2023/06/13/LYKqpsglET7nU3W.png" alt="image.png" style="zoom: 50%;" /></p>
<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p>我们有$R(\beta) = \frac{1}{2n}||(y-X\beta)||^2 = \frac{1}{2n}(y-X\beta)^T(y-X\beta)$，求偏导有</p>
<script type="math/tex; mode=display">\frac{\partial R}{\partial \beta}=-\frac{1}{n}X^T(y-X\beta),\ \frac{\partial^2R}{\partial \beta^2}=\frac{1}{n}X^TX</script><p>二阶偏导为正，说明正定，令一阶偏导为0求得$\beta_\min = (X^TX)^{-1}X^Ty$</p>
<p>优点：不用定义收敛率，也不用迭代</p>
<p>缺点：需要保证$X^TX$可逆；当维数$d$很高的时候求逆是$O(d^3)$的，很慢。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>同时让$\beta$的每个分量沿着梯度下降：</p>
<p>$\beta_i = \beta_i-\alpha \frac{\partial R}{\partial \beta_i}$，其中$\alpha$是学习率。</p>
<p>优点：高维依然很管用</p>
<p>缺点：需要迭代，有时候迭代很多代才收敛；需要调参$\alpha$</p>
<h4 id="实际考虑"><a href="#实际考虑" class="headerlink" title="实际考虑"></a>实际考虑</h4><ul>
<li>一般会把数据做个归一化，把数据变小点</li>
<li>学习率不能太大，也不能太小</li>
<li>损失函数应该随着迭代递减</li>
<li>如果优化程度在$\epsilon$之内说明收敛了</li>
<li>什么时候$X^TX$不可逆？<ul>
<li>样本过少，特征过多</li>
<li>特征之间线性相关</li>
</ul>
</li>
</ul>
<h3 id="对数回归"><a href="#对数回归" class="headerlink" title="对数回归"></a>对数回归</h3><p>做分类，线性回归也不是不行，只是表现一般，而且预测值有时候又不在$[0, 1]$之间。</p>
<p>对数回归不是一个回归方法，而是一个分类方法，虽然怪怪的但是课件原话。</p>
<script type="math/tex; mode=display">g(x) = \text{SIGMOID}(z) = \frac{e^z}{1+e^z}</script><p><img src="https://s2.loli.net/2023/06/13/xFzXuBYUosTMe4w.png" alt="image.png" style="zoom:50%;" /></p>
<p>对数回归用的$f$就不太一样了，相当于是把输出映射到了$[0,1]$</p>
<script type="math/tex; mode=display">f(x)=g(\beta_0+\sum\limits_{j=1}^d\beta_jx_j),\ R=\frac{1}{2n}\sum\limits_{i=1}^n(y_i-f(x_i))^2</script><p>分类的时候，离哪边近就分到哪边。</p>
<p>旧损失函数$\text{Loss}=\frac 1 2(f(x)-y)^2$这个时候就不是简单的二次函数了，可能有很多局部最优，所以对旧损失函数的梯度下降寄了。为了全局最优，对损失使用凸函数罢！</p>
<script type="math/tex; mode=display">\text{Cost}(f(x),y)=\left\{\begin{align*}&-\log(f(x))\quad&\text{if } y = 1\\&-\log(1-f(x))\quad&\text{if } y = 0\end{align*}\right.</script><p>所以有损失函数：</p>
<script type="math/tex; mode=display">\text{Loss}(f(x),y)=-y\log(f(x))-(1-y)\log(1-f(x))</script><script type="math/tex; mode=display">R(\beta)=\frac{1}{n}\sum\limits_{i=1}^n\text{Loss}_i</script><p>好了，我们又可以梯度下降了。</p>
<h2 id="Chapter-9-支持向量机"><a href="#Chapter-9-支持向量机" class="headerlink" title="Chapter 9 - 支持向量机"></a>Chapter 9 - 支持向量机</h2><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p>线性模型的分类靠超平面：$f(x)=\left \langle w,x\right \rangle+b=0$</p>
<p>支持向量机要求所有点都在正负超平面之外(含超平面)，正负超平面：$\left \langle w,x\right \rangle+b=±c$</p>
<p> 不过我们可以通过放缩变成：$\left \langle w,x\right \rangle+b=±1$</p>
<p>在正负超平面上的俩决策临界点连线垂直三个超平面，所以有$x^+-x^-=\lambda w$</p>
<p>又有$\lambda=\frac{2}{||w||_2^2}$，正负超平面之间的距离就是$M=\frac 2 {||w||_2}$</p>
<p>现在的目标就是最大化$M$，满足</p>
<script type="math/tex; mode=display">\left \langle w,x_i\right \rangle+b\left\{\begin{align*}&\ge 1\quad &\text{if } y_i&=1\\&\le -1\quad &\text{if } y_i&=-1\end{align*}\right.</script><p>再换句话，就是求$w,b$来达到$\min\limits_w\frac{||w||^2_2}{2}$，条件是$y_i(\left \langle w,x_i\right \rangle+b)\ge 1$</p>
<p><del>过程太复杂不想写了aaaaa</del></p>
<p>等明天醒了补！</p>
<p><strong>解题步骤</strong></p>
<ul>
<li>解对偶优化问题，算出$\alpha^\star$</li>
<li>用$\alpha^\star$算出$w(\alpha^\star)$和$b(\alpha^\star)$</li>
<li>用这个式子分类：$\text{sign}(\sum\limits_{i=1}^n\alpha_i^\star y_ik(x_i,x)+b(\alpha^\star))$</li>
</ul>
<h3 id="非线性模型"><a href="#非线性模型" class="headerlink" title="非线性模型"></a>非线性模型</h3><p>把输入空间映射到特征空间，特征空间维数越高，数据越可能线性可分。</p>
<p>映射完之后该咋做咋做就好了，映射用的函数叫核函数。</p>
<p>线性核，多项式核，高斯核</p>
<h2 id="Chapter-10-感知机与神经网络"><a href="#Chapter-10-感知机与神经网络" class="headerlink" title="Chapter 10 - 感知机与神经网络"></a>Chapter 10 - 感知机与神经网络</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>$f(x_i)=\text{sign}(\sum\limits_{j=0}^dw_jx_{ij})$</p>
<p>如果数据线性可分那就好办，否则不收敛</p>
<p>从一个随机的超平面开始，用训练数据来调整的迭代方法</p>
<p>发现$y_if(x_i)\le 0$时，更新全部$w_j=w_j+y_ix_{ij}$</p>
<p>感性理解，就是把每个分量拉回来对应的$x$值</p>
<p>$w_j$决定了$x_j$对结果的权重，可能得到很多解</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>感知机用的是分段函数，换成SIGMOID，也可以换成别的激活函数比如tanh</p>
<p>简单的例子：用OR，NAND，AND感知机拼成XOR网络</p>
<h4 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h4><p>明天醒的时候再看</p>
<h2 id="Chapter-11-决策树和朴素贝叶斯"><a href="#Chapter-11-决策树和朴素贝叶斯" class="headerlink" title="Chapter 11 - 决策树和朴素贝叶斯"></a>Chapter 11 - 决策树和朴素贝叶斯</h2><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>还是做分类的。不需要实数输入，所以不需要数值化特征。</p>
<ol>
<li>主要的决策：下一个用哪个标准分类？</li>
<li>用同质性作为衡量标准：如果分类两边样本数量相等同质性最高，如果分完只剩一类了那就最低。</li>
<li>熵函数 $\text{Entropy}(S)=-p_+\log p_+ -p_-\log p_-$是个关于x=0.5对称的凸函数</li>
</ol>
<p>收获函数$\text{Gain}(S,A)=\text{Entropy}(S)-\sum\limits_{v\in\text{Value(A)}}\frac{|S_v|}{|S|}\text{Entropy}(S_v)$</p>
<p>把子结点人数占比作为权重，子结点熵值作为权值，父节点熵值减去子结点熵值的加权平均就是熵值收获。</p>
<p>很容易过拟合，两种解法：要么别长出来，要么先长出来再剪(这个更好，用一个验证集检验剪枝后有没有比之前更差)。</p>
<p><strong>CART</strong>用$Gini=1-p_+^2-p_-^2$代替信息熵。</p>
<p><strong>实际问题</strong></p>
<ul>
<li>先降维，保留特征性最强的特征</li>
<li>用集成方法：随机森林</li>
<li>先均衡数据集：对主要数据欠采样，对少数数据过采样</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>简单可解释</li>
<li>可以转成分类规则</li>
<li>对于分类数据很好</li>
<li>构造简单</li>
<li>不用归一化数据</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>不稳定，一个样本不同树可能就不同</li>
<li>单变量，不处理组合特征</li>
<li>某些结点的选择可能取决于之前的选择</li>
<li>需要平衡数据</li>
</ul>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><script type="math/tex; mode=display">p(A|B)=\frac{p(B|A)*p(A)}{p(B)}=\frac{p(B|A)*p(A)}{p(B|\overline A)p(\overline A)+p(B|A)*p(A)}</script><p>希望通过$p(y|x)$来找到$f(x)=y$的映射。</p>
<h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><p>对$p(y|x)$建模，然后给条决策边界来分类。</p>
<h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><p>对$p(x|y)$和$p(y)$建模，然后用贝叶斯算使得$p(y|x)=\frac{p(x|y)p(y)}{p(x)}≈p(x|y)p(y)$最大的$y$值作为结果。</p>
<p>输入$x$维数比较高的时候，可以默认不同维特征之间相互独立。</p>
<p><strong>解题步骤</strong></p>
<ul>
<li>对每个标签$y$算$p(y)$</li>
<li>对每个标签$y$和每个特征$a_i$算$p(a_i|y)$</li>
</ul>
<p>$m-$估计概率：$p(a_j|y)=\frac{n_c+m*p}{n_y+m}$</p>
<p>其中$p$是事先约定的值，不知道怎么取可以用特征$y$可能的取值数量的倒数</p>
<h2 id="Chapter-12-集成学习与聚类"><a href="#Chapter-12-集成学习与聚类" class="headerlink" title="Chapter 12 - 集成学习与聚类"></a>Chapter 12 - 集成学习与聚类</h2><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>用同样的训练数据，训练独立的弱学习者，策略有三：</p>
<ul>
<li>Boosting</li>
<li>Bagging</li>
<li>Random Forests</li>
</ul>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p>在带权重的训练数据上训练弱学习者，表现好的学习者给更多权重</p>
<p>难样本给更高权重，简单样本给更低权重</p>
<p>错误率决定了最终的贡献比：</p>
<script type="math/tex; mode=display">\alpha_m=\frac 1 2 \log(\frac{1-err_m}{err_m})</script><p><img src="https://s2.loli.net/2023/06/14/tpjgqJIMOQrNUAb.png" alt="image.png" style="zoom: 33%;" /></p>
<p>数据集上的权重从均权开始，错的人越多权值变得越大，越少就变小</p>
<script type="math/tex; mode=display">w_i=w_i*\exp[-\alpha_my_iG_m(x_i)]</script><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>Bootstrapping是重采样，按照经验分布进行采样</p>
<p>Bagging和Boosting都基于Bootstrapping</p>
<p>都使用了重采样来生成弱分类器</p>
<p>Bagging = Bootstrap aggregation</p>
<p>每次就选等大的一部分数据用来训练一个弱分类器。</p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>不拆数据了，拆特征维数，每次只用一部分特征来分类，多长几棵树投票。</p>
<p>一般拆出来训练的特征维数$m\le\sqrt d$</p>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>无监督学习，分成$k$个聚类</p>
<h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><p>最小化聚类内部的均方差：</p>
<script type="math/tex; mode=display">J=\sum\limits_{j=1}^k\sum\limits_{x_i\in C_j}||x_i-\mu_j||^2</script><p>$\mu_j$是第$j$个聚类中的平均值</p>
<p>优点：容易实现</p>
<p>缺点：得知道$K$；维数灾难；没有理论基础</p>
<h4 id="K-means的一些问题"><a href="#K-means的一些问题" class="headerlink" title="K-means的一些问题"></a>K-means的一些问题</h4><p><strong>怎么取$k$？</strong></p>
<p>多试几个，选”elbow-point”</p>
<p>G-means算法</p>
<ol>
<li>用一个小$k$开始</li>
<li>从$k$个聚类中心点跑K-means</li>
<li>看看聚类是否环绕这些中心正态分布</li>
<li>如果OK就不管，如果不是那就拆成两个聚类中心</li>
<li>继续迭代，直到大家都正态分布</li>
</ol>
<p><strong>怎么评价模型？</strong></p>
<p>内部评价：高聚类内部相似度，低聚类外部相似度数据检验，可以用 Davies-Bouldin index评估聚类的紧密型</p>
<p>外部评价：利用对外部数据的知识库做评估(是否符合客观规律)</p>
<p><strong>如果不想要圆形聚类呢？</strong></p>
<p>其他方法：spectral clustering, kernelized K-means, DBSCAN, BIRCH, etc.</p>
<p><strong>怎么初始化？</strong></p>
<p>K-means对聚类中心的选取高度敏感，收敛速度和聚类效果都有影响。</p>
<p>比较安全的技巧是新聚类中心和之前的聚类中心安排得尽量远</p>
<p>解决方案是多重开几次选最好的。</p>
<p><strong>其他的限制？</strong></p>
<p>对一些样本做硬赋值，高斯混合模型允许软赋值(不同的赋值有概率)</p>
<p>对离群的例子高敏感，K-median在这方面的鲁棒性更好</p>
<script type="math/tex; mode=display">\Huge \mathscr{Good\ luck!}</script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Artificial-Intelligence/" rel="tag"># Artificial Intelligence</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/06/05/CS216_Final_Review/" rel="prev" title="CS216 算法设计与分析 期末复习">
      <i class="fa fa-chevron-left"></i> CS216 算法设计与分析 期末复习
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/06/23/Fisher_Info_Matrix/" rel="next" title="Fisher Information Matrix">
      Fisher Information Matrix <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <div id="gitalk-container"></div>
  <script>
  const gitalk = new Gitalk({
    clientID: '79118b0b29b4170f24af',
    clientSecret: '48ee47026d13b8a3a954b2b089ea320510282527',
    repo: 'GuTaoZi.github.io',      // The repository of store comments,
    owner: 'GuTaoZi',
    admin: ['GuTaoZi'],
    id: location.pathname,      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })
  gitalk.render('gitalk-container')
  </script>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">Chapter 1 - 人工智能概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-number">1.1.</span> <span class="nav-text">智能体</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83"><span class="nav-number">1.2.</span> <span class="nav-text">环境</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-2-%E6%90%9C%E7%B4%A2"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2 - 搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E6%A6%82%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">搜索概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E4%BF%A1%E6%81%AF%E6%90%9C%E7%B4%A2"><span class="nav-number">2.2.</span> <span class="nav-text">无信息搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E4%BF%A1%E6%81%AF%E6%90%9C%E7%B4%A2"><span class="nav-number">2.3.</span> <span class="nav-text">有信息搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83BFS"><span class="nav-number">2.3.1.</span> <span class="nav-text">贪心BFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-%E6%90%9C%E7%B4%A2"><span class="nav-number">2.3.2.</span> <span class="nav-text">A*搜索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E6%8E%A5%E5%8F%97%E7%9A%84%E5%90%AF%E5%8F%91%E5%87%BD%E6%95%B0-Admissible-heuristics"><span class="nav-number">2.3.3.</span> <span class="nav-text">可接受的启发函数(Admissible heuristics)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#IDA-%E6%90%9C%E7%B4%A2"><span class="nav-number">2.3.4.</span> <span class="nav-text">IDA*搜索</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2"><span class="nav-number">2.4.</span> <span class="nav-text">局部搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%88%AC%E5%B1%B1%E7%AE%97%E6%B3%95"><span class="nav-number">2.4.1.</span> <span class="nav-text">爬山算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="nav-number">2.4.2.</span> <span class="nav-text">遗传算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB"><span class="nav-number">2.4.3.</span> <span class="nav-text">模拟退火</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-3-%E5%90%AF%E5%8F%91%E4%B8%8E%E5%85%83%E5%90%AF%E5%8F%91"><span class="nav-number">3.</span> <span class="nav-text">Chapter 3 - 启发与元启发</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6"><span class="nav-number">3.1.</span> <span class="nav-text">启发式函数进阶</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E4%B8%8E%E6%95%88%E7%8E%87"><span class="nav-number">3.1.1.</span> <span class="nav-text">启发式函数与效率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%BD%E7%9A%84%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E4%B8%8A%E5%93%AA%E6%89%BE%EF%BC%9F"><span class="nav-number">3.1.2.</span> <span class="nav-text">好的启发式函数上哪找？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%83%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6"><span class="nav-number">3.2.</span> <span class="nav-text">元启发式函数进阶</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-4-%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2"><span class="nav-number">4.</span> <span class="nav-text">Chapter 4 - 对抗搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Minimax%E6%90%9C%E7%B4%A2"><span class="nav-number">4.1.</span> <span class="nav-text">Minimax搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alpha-beta-%E5%89%AA%E6%9E%9D"><span class="nav-number">4.2.</span> <span class="nav-text">$\alpha-\beta$ 剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E8%A6%81%E5%89%AA%E5%91%A2%EF%BC%9F"><span class="nav-number">4.2.1.</span> <span class="nav-text">什么时候要剪呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%80%9C%E8%87%AA%E8%A3%81%E5%BC%8F%E2%80%9D%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.2.2.</span> <span class="nav-text">“自裁式”实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%A2%E5%BA%8F%E5%89%AA%E6%9E%9D"><span class="nav-number">4.2.3.</span> <span class="nav-text">换序剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E6%83%85%E5%86%B5"><span class="nav-number">4.2.4.</span> <span class="nav-text">实际情况</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9BMinimax%E6%90%9C%E7%B4%A2"><span class="nav-number">4.3.</span> <span class="nav-text">期望Minimax搜索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-5-%E7%BA%A6%E6%9D%9F%E6%BB%A1%E8%B6%B3%E9%97%AE%E9%A2%98"><span class="nav-number">5.</span> <span class="nav-text">Chapter 5 - 约束满足问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CSP%E6%A6%82%E8%BF%B0"><span class="nav-number">5.1.</span> <span class="nav-text">CSP概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3CSP"><span class="nav-number">5.2.</span> <span class="nav-text">解决CSP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9BBTS%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-number">5.3.</span> <span class="nav-text">改进BTS的三种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E5%89%A9%E4%BD%99%E5%80%BC-Minimum-Remaining-Value"><span class="nav-number">5.3.1.</span> <span class="nav-text">最小剩余值(Minimum Remaining Value)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E9%99%90%E5%88%B6%E5%80%BC-Least-Constraining-Value"><span class="nav-number">5.3.2.</span> <span class="nav-text">最小限制值(Least Constraining Value)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E6%A3%80%E6%9F%A5-Forward-Checking"><span class="nav-number">5.3.3.</span> <span class="nav-text">前向检查(Forward Checking)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">5.4.</span> <span class="nav-text">一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%82%B9%E4%B8%80%E8%87%B4%E6%80%A7-%E4%B8%80%E5%85%83%E9%99%90%E5%88%B6"><span class="nav-number">5.4.1.</span> <span class="nav-text">点一致性(一元限制)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A7%E4%B8%80%E8%87%B4%E6%80%A7-%E4%BA%8C%E5%85%83%E9%99%90%E5%88%B6"><span class="nav-number">5.4.2.</span> <span class="nav-text">弧一致性(二元限制)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E4%B8%80%E8%87%B4%E6%80%A7-%E5%A4%9A%E5%85%83%E9%99%90%E5%88%B6"><span class="nav-number">5.4.3.</span> <span class="nav-text">路径一致性(多元限制)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-6-%E9%80%BB%E8%BE%91"><span class="nav-number">6.</span> <span class="nav-text">Chapter 6 - 逻辑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%93%BA%E5%9E%AB"><span class="nav-number">6.1.</span> <span class="nav-text">铺垫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Resolution"><span class="nav-number">6.2.</span> <span class="nav-text">Resolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-Backward-chaining"><span class="nav-number">6.3.</span> <span class="nav-text">Forward &#x2F; Backward chaining</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-7-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-number">7.</span> <span class="nav-text">Chapter 7 - 机器学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.1.</span> <span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.2.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K%E8%BF%91%E9%82%BB"><span class="nav-number">7.3.</span> <span class="nav-text">K近邻</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-Validation-and-Test"><span class="nav-number">7.4.</span> <span class="nav-text">Train, Validation and Test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">7.5.</span> <span class="nav-text">过拟合和欠拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-Validation-and-Test-1"><span class="nav-number">7.6.</span> <span class="nav-text">Train, Validation and Test</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-fold%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">7.6.1.</span> <span class="nav-text">K-fold交叉验证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5-Confusion-matrix"><span class="nav-number">7.6.2.</span> <span class="nav-text">混淆矩阵(Confusion matrix)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%AF%B9%E6%95%B0%E5%9B%9E%E5%BD%92"><span class="nav-number">8.</span> <span class="nav-text">Chapter 8 - 线性回归与对数回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">8.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="nav-number">8.1.1.</span> <span class="nav-text">正规方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">8.1.2.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E8%80%83%E8%99%91"><span class="nav-number">8.1.3.</span> <span class="nav-text">实际考虑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E5%9B%9E%E5%BD%92"><span class="nav-number">8.2.</span> <span class="nav-text">对数回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-9-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">9.</span> <span class="nav-text">Chapter 9 - 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.1.</span> <span class="nav-text">线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.2.</span> <span class="nav-text">非线性模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-10-%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">10.</span> <span class="nav-text">Chapter 10 - 感知机与神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">10.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">10.2.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="nav-number">10.2.1.</span> <span class="nav-text">反向传播算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-11-%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">11.</span> <span class="nav-text">Chapter 11 - 决策树和朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">11.1.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">11.2.</span> <span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="nav-number">11.2.1.</span> <span class="nav-text">判别模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">11.2.2.</span> <span class="nav-text">生成模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-12-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%81%9A%E7%B1%BB"><span class="nav-number">12.</span> <span class="nav-text">Chapter 12 - 集成学习与聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">12.1.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Boosting"><span class="nav-number">12.1.1.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bagging"><span class="nav-number">12.1.2.</span> <span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">12.1.3.</span> <span class="nav-text">随机森林</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">12.2.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means"><span class="nav-number">12.2.1.</span> <span class="nav-text">K-means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-number">12.2.2.</span> <span class="nav-text">K-means的一些问题</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="咕桃"
      src="https://avatars.githubusercontent.com/u/109007949?v=4">
  <p class="site-author-name" itemprop="name">咕桃</p>
  <div class="site-description" itemprop="description">Just Do It, But Not Just Do It.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/GuTaoZi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;GuTaoZi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:first_fan@outlook.com" title="E-Mail → mailto:first_fan@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">咕桃</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
